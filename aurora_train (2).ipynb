{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls /datasets/aurora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "Package 'python3-scipy' is not installed, so not removed\n",
      "Package 'python3-numpy' is not installed, so not removed\n",
      "The following packages were automatically installed and are no longer required:\n",
      "  cudnn-license fonts-lyx libabsl20210324 libaec0 libboost-dev\n",
      "  libboost1.74-dev libc-ares2 libdouble-conversion3 libgif7 libhdf5-103-1\n",
      "  libhdf5-hl-100 libimagequant0 libjs-jquery-ui liblbfgsb0 liblzf1 libmagma2\n",
      "  libopenblas-dev libopenblas-pthread-dev libopenblas0 libqhull-r8.0 libraqm0\n",
      "  libsz2 libwebpdemux2 libxsimd-dev python-matplotlib-data python3-absl\n",
      "  python3-appdirs python3-astunparse python3-beniget python3-bleach\n",
      "  python3-brotli python3-bs4 python3-cycler python3-einops python3-flatbuffers\n",
      "  python3-fs python3-fsspec python3-gast python3-grpcio python3-html5lib\n",
      "  python3-joblib python3-kiwisolver python3-lxml python3-lz4 python3-mpmath\n",
      "  python3-namex python3-networkx python3-olefile python3-optree python3-pasta\n",
      "  python3-pil python3-pil.imagetk python3-protobuf python3-soupsieve\n",
      "  python3-sympy python3-termcolor python3-threadpoolctl python3-unicodedata2\n",
      "  python3-webencodings python3-werkzeug python3-wrapt unicode-data\n",
      "Use 'sudo apt autoremove' to remove them.\n",
      "0 upgraded, 0 newly installed, 0 to remove and 2 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "! sudo apt remove -y python3-numpy python3-scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BIcWnWHS6aUa",
    "outputId": "93577fbb-3835-4855-88e2-90a327fb92c4",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://download.pytorch.org/whl/cu124\n",
      "Requirement already satisfied: torch==2.5.0 in /home/ubuntu/.local/lib/python3.10/site-packages (2.5.0)\n",
      "Requirement already satisfied: torchvision==0.20.0 in /home/ubuntu/.local/lib/python3.10/site-packages (0.20.0)\n",
      "Requirement already satisfied: torchaudio==2.5.0 in /home/ubuntu/.local/lib/python3.10/site-packages (2.5.0)\n",
      "Requirement already satisfied: jinja2 in /usr/lib/python3/dist-packages (from torch==2.5.0) (3.0.3)\n",
      "Requirement already satisfied: networkx in /usr/lib/python3/dist-packages (from torch==2.5.0) (2.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch==2.5.0) (1.13.1)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3/dist-packages (from torch==2.5.0) (3.6.0)\n",
      "Requirement already satisfied: fsspec in /usr/lib/python3/dist-packages (from torch==2.5.0) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/lib/python3/dist-packages (from torch==2.5.0) (4.9.0)\n",
      "Requirement already satisfied: numpy in /home/ubuntu/.local/lib/python3.10/site-packages (from torchvision==0.20.0) (1.26.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/lib/python3/dist-packages (from torchvision==0.20.0) (9.0.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from sympy==1.13.1->torch==2.5.0) (1.3.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting cdsapi\n",
      "  Using cached cdsapi-0.7.5-py2.py3-none-any.whl (12 kB)\n",
      "Collecting dask\n",
      "  Using cached dask-2024.12.1-py3-none-any.whl (1.3 MB)\n",
      "Requirement already satisfied: xarray in /home/ubuntu/.local/lib/python3.10/site-packages (2025.1.1)\n",
      "Collecting netcdf4\n",
      "  Downloading netCDF4-1.7.2-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (9.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.2/9.2 MB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.0-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (8.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m163.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "Collecting microsoft-aurora\n",
      "  Downloading microsoft_aurora-1.4.1-py3-none-any.whl (187 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m187.1/187.1 KB\u001b[0m \u001b[31m67.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting boto3\n",
      "  Downloading boto3-1.35.97-py3-none-any.whl (139 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.2/139.2 KB\u001b[0m \u001b[31m51.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting awscli\n",
      "  Downloading awscli-1.36.38-py3-none-any.whl (4.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m271.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting datapi\n",
      "  Downloading datapi-0.1.2-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: requests>=2.5.0 in /usr/lib/python3/dist-packages (from cdsapi) (2.25.1)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 KB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting partd>=1.4.0\n",
      "  Downloading partd-1.4.2-py3-none-any.whl (18 kB)\n",
      "Collecting click>=8.1\n",
      "  Downloading click-8.1.8-py3-none-any.whl (98 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.2/98.2 KB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting cloudpickle>=3.0.0\n",
      "  Downloading cloudpickle-3.1.0-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /usr/lib/python3/dist-packages (from dask) (5.4.1)\n",
      "Collecting toolz>=0.10.0\n",
      "  Downloading toolz-1.0.0-py3-none-any.whl (56 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.4/56.4 KB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: fsspec>=2021.09.0 in /usr/lib/python3/dist-packages (from dask) (2024.3.1)\n",
      "Collecting importlib_metadata>=4.13.0\n",
      "  Downloading importlib_metadata-8.5.0-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from dask) (24.2)\n",
      "Requirement already satisfied: numpy>=1.24 in /home/ubuntu/.local/lib/python3.10/site-packages (from xarray) (1.26.3)\n",
      "Requirement already satisfied: pandas>=2.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from xarray) (2.2.3)\n",
      "Collecting cftime\n",
      "  Downloading cftime-1.6.4.post1-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m149.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: certifi in /usr/lib/python3/dist-packages (from netcdf4) (2020.6.20)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/ubuntu/.local/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.55.3-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (4.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m205.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting contourpy>=1.0.1\n",
      "  Downloading contourpy-1.3.1-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (312 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m312.0/312.0 KB\u001b[0m \u001b[31m68.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: kiwisolver>=1.3.1 in /usr/lib/python3/dist-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/lib/python3/dist-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: pillow>=8 in /usr/lib/python3/dist-packages (from matplotlib) (9.0.1)\n",
      "Requirement already satisfied: scipy in /home/ubuntu/.local/lib/python3.10/site-packages (from microsoft-aurora) (1.15.1)\n",
      "Requirement already satisfied: torch in /home/ubuntu/.local/lib/python3.10/site-packages (from microsoft-aurora) (2.5.0)\n",
      "Collecting huggingface-hub\n",
      "  Downloading huggingface_hub-0.27.1-py3-none-any.whl (450 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m450.7/450.7 KB\u001b[0m \u001b[31m143.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting timm==0.6.13\n",
      "  Downloading timm-0.6.13-py3-none-any.whl (549 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m549.1/549.1 KB\u001b[0m \u001b[31m155.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: einops in /usr/lib/python3/dist-packages (from microsoft-aurora) (0.8.0)\n",
      "Requirement already satisfied: torchvision in /home/ubuntu/.local/lib/python3.10/site-packages (from timm==0.6.13->microsoft-aurora) (0.20.0)\n",
      "Collecting botocore<1.36.0,>=1.35.97\n",
      "  Downloading botocore-1.35.97-py3-none-any.whl (13.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m272.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "Collecting s3transfer<0.11.0,>=0.10.0\n",
      "  Downloading s3transfer-0.10.4-py3-none-any.whl (83 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.2/83.2 KB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting jmespath<2.0.0,>=0.7.1\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Collecting rsa<4.8,>=3.1.2\n",
      "  Downloading rsa-4.7.2-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: colorama<0.4.7,>=0.2.5 in /usr/lib/python3/dist-packages (from awscli) (0.4.4)\n",
      "Collecting docutils<0.17,>=0.10\n",
      "  Downloading docutils-0.16-py2.py3-none-any.whl (548 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m548.2/548.2 KB\u001b[0m \u001b[31m143.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/lib/python3/dist-packages (from botocore<1.36.0,>=1.35.97->boto3) (1.26.5)\n",
      "Collecting zipp>=3.20\n",
      "  Downloading zipp-3.21.0-py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas>=2.1->xarray) (2022.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ubuntu/.local/lib/python3.10/site-packages (from pandas>=2.1->xarray) (2024.2)\n",
      "Collecting locket\n",
      "  Downloading locket-1.0.0-py2.py3-none-any.whl (4.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /usr/lib/python3/dist-packages (from rsa<4.8,>=3.1.2->awscli) (0.4.8)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch->microsoft-aurora) (1.13.1)\n",
      "Requirement already satisfied: jinja2 in /usr/lib/python3/dist-packages (from torch->microsoft-aurora) (3.0.3)\n",
      "Requirement already satisfied: networkx in /usr/lib/python3/dist-packages (from torch->microsoft-aurora) (2.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/lib/python3/dist-packages (from torch->microsoft-aurora) (4.9.0)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3/dist-packages (from torch->microsoft-aurora) (3.6.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from sympy==1.13.1->torch->microsoft-aurora) (1.3.0)\n",
      "Collecting multiurl>=0.3.2\n",
      "  Downloading multiurl-0.3.3.tar.gz (18 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: attrs in /usr/lib/python3/dist-packages (from datapi->cdsapi) (21.2.0)\n",
      "Building wheels for collected packages: multiurl\n",
      "  Building wheel for multiurl (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for multiurl: filename=multiurl-0.3.3-py3-none-any.whl size=21245 sha256=d56acab32014b4f1254558969935c2bc4e6de96bbbbfe2c33cd626bc6567e3c9\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/be/05/e0/65a6edb0a000498aeaefbadd80228bf5ed1bdbb82840ca1692\n",
      "Successfully built multiurl\n",
      "Installing collected packages: zipp, tqdm, toolz, rsa, locket, jmespath, fonttools, docutils, contourpy, cloudpickle, click, cftime, partd, netcdf4, multiurl, matplotlib, importlib_metadata, huggingface-hub, botocore, s3transfer, datapi, dask, timm, cdsapi, boto3, awscli, microsoft-aurora\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pipx 1.0.0 requires argcomplete>=1.9.4, but you have argcomplete 1.8.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed awscli-1.36.38 boto3-1.35.97 botocore-1.35.97 cdsapi-0.7.5 cftime-1.6.4.post1 click-8.1.8 cloudpickle-3.1.0 contourpy-1.3.1 dask-2024.12.1 datapi-0.1.2 docutils-0.16 fonttools-4.55.3 huggingface-hub-0.27.1 importlib_metadata-8.5.0 jmespath-1.0.1 locket-1.0.0 matplotlib-3.10.0 microsoft-aurora-1.4.1 multiurl-0.3.3 netcdf4-1.7.2 partd-1.4.2 rsa-4.7.2 s3transfer-0.10.4 timm-0.6.13 toolz-1.0.0 tqdm-4.67.1 zipp-3.21.0\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scikit-learn in /home/ubuntu/.local/lib/python3.10/site-packages (1.6.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/lib/python3/dist-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from scikit-learn) (1.15.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /home/ubuntu/.local/lib/python3.10/site-packages (from scikit-learn) (1.26.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==2.5.0 torchvision==0.20.0 torchaudio==2.5.0 --index-url https://download.pytorch.org/whl/cu124\n",
    "!pip install cdsapi dask xarray netcdf4 matplotlib microsoft-aurora boto3 awscli\n",
    "!export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\n",
    "!pip install scikit-learn\n",
    "# !pip install --upgrade numpy scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tthe next two boxes are used for using aws cli "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aws-cli/1.36.22 Python/3.10.12 Linux/6.8.0-1013-nvidia-64k botocore/1.35.81\n"
     ]
    }
   ],
   "source": [
    "!aws --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see awsid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "from tqdm import tqdm\n",
    "from aurora import Batch, Metadata\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import boto3\n",
    "from io import BytesIO\n",
    "import io\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "from aurora import Aurora\n",
    "from aurora.normalisation import locations, scales\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import random_split\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import gc\n",
    "# from netCDF4 import Dataset\n",
    "# import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# see awsid for missing line\n",
    "\n",
    "# bucket_name = 'globfire-gooddata'\n",
    "# compression_settings = {'zlib': True, 'complevel': 3}\n",
    "\n",
    "# surf_comb = xr.open_mfdataset(surf_files, combine='nested')\n",
    "# encoding_surf = {var: compression_settings for var in surf_comb.data_vars}\n",
    "# surf_comb.to_netcdf(\"surf_comb.nc\", encoding=encoding_surf)\n",
    "# s3.upload_file(\"surf_comb.nc\", bucket_name, \"surf_comb.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code for joinging 3 days worth of data together to feed into aurora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "download_path = \"datasets/aurora\"\n",
    "surf_path = f\"../{download_path}/fle\"\n",
    "atmos_path = f\"../{download_path}/atmospheric\"\n",
    "\n",
    "static_vars_ds = xr.open_dataset(f\"../{download_path}/static/static.nc\", engine=\"netcdf4\")\n",
    "atmos_files = glob.glob(f\"../{download_path}/atmospheric/*/*.nc\") \n",
    "atmos_files_2021 = [f for f in atmos_files if int(f.split('/')[-2]) <= 202112]\n",
    "surf_files = glob.glob(f\"../{download_path}/fle/*.nc\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "starty = 2015     ## start of 2015 to end of 2021\n",
    "endy = 2021           \n",
    "date_range = pd.date_range(start=f'{starty}-01-01', end=f'{endy}-12-31', freq='D')\n",
    "\n",
    "download_path = \"datasets/aurora\"\n",
    "surf_path = f\"../{download_path}/fle\"\n",
    "atmos_path = f\"../{download_path}/atmospheric\"\n",
    "\n",
    "atmos_files = glob.glob(f\"../{download_path}/atmospheric/*/*.nc\") \n",
    "# atmos_files = glob.glob(f\"../{download_path}/atmospheric/(201[0-9][0-9]{2}|202(0|1)[0-9]{2})/*.nc\")  \n",
    "atmos_files_2021 = [f for f in atmos_files if int(f.split('/')[-2]) <= 202112]\n",
    "surf_files = glob.glob(f\"../{download_path}/fle/*.nc\") \n",
    "\n",
    "\n",
    "days = 3\n",
    "\n",
    "# ./datasets/aurora/atmospheric/201501/atmospheric_20150111.nc'\n",
    "#../datasets/aurora/fle/surf_2021-11-17.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "i=0\n",
    "date_range[i:i + days]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## basic one day batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "data loading:   0%|          | 0/3 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'atmos_files_2021' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6270/3452403814.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0msurf_regex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34mrf\".*/surf_{date_window[day].strftime('%Y-%m-%d')}\\.nc$\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0matmos_filt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0matmos_files_2021\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0matmos_regex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0msurf_filt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msurf_files\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msurf_regex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'atmos_files_2021' is not defined"
     ]
    }
   ],
   "source": [
    "variables_to_keep = ['u10', 'v10', 't2m', 'msl', 'lst', 'fire']\n",
    "date_range = pd.date_range(start=f'{2015}-01-01', end=f'{2015}-1-03', freq='D')\n",
    "days = 1\n",
    "for i in tqdm(range(len(date_range) - days + 1), desc = \"data loading\"):\n",
    "    date_window = date_range[i:i + days]\n",
    "    \n",
    "    atmos_regex = []\n",
    "    surf_regex = []\n",
    "    for day in range(days):\n",
    "        atmos_regex.append( re.compile( rf\".*/{date_window[day].strftime('%Y%m')}/atmospheric_{date_window[day].strftime('%Y%m%d')}\\.nc$\"))\n",
    "        surf_regex.append( re.compile( rf\".*/surf_{date_window[day].strftime('%Y-%m-%d')}\\.nc$\"))\n",
    "    \n",
    "    atmos_filt = [f for f in atmos_files_2021 if any(d.match(f) for d in atmos_regex)]\n",
    "    surf_filt = [f for f in surf_files if any(d.match(f) for d in surf_regex)]\n",
    "\n",
    "    print(atmos_filt)\n",
    "    print(surf_filt)\n",
    "    \n",
    "    \n",
    "    atmos_list = []\n",
    "    surf_list = []\n",
    "    \n",
    "    for day in range(len(atmos_filt)):\n",
    "        atmos_cur = xr.open_dataset(atmos_filt[day])\n",
    "        surf_cur = xr.open_dataset(surf_filt[day])\n",
    "        \n",
    "        # code for dropping variables. makes code run slower. interpolation so fast that it doest matter. \n",
    "        surf_cur = surf_cur.drop_vars([var for var in surf_cur.data_vars if var not in variables_to_keep])\n",
    "\n",
    "        atmos_list.append(atmos_cur)\n",
    "        surf_list.append(surf_cur)\n",
    "    \n",
    "    atmos_comb = xr.concat(atmos_list, dim=\"valid_time\")\n",
    "    surf_comb = xr.concat(surf_list, dim=\"valid_time\")\n",
    "    \n",
    "static_vars_ds = xr.open_dataset(f\"../{download_path}/static/static.nc\", engine=\"netcdf4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_PwqH22r8h8L",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "i = 1  # Select this time index in the downloaded data.\n",
    "\n",
    "batch = Batch(\n",
    "    surf_vars={\n",
    "        # First select time points `i` and `i - 1`. Afterwards, `[None]` inserts a\n",
    "        # batch dimension of size one.\n",
    "        \"2t\": torch.from_numpy(surf_comb[\"t2m\"].values[[i - 1, i]][None]),\n",
    "        \"10u\": torch.from_numpy(surf_comb[\"u10\"].values[[i - 1, i]][None]),\n",
    "        \"10v\": torch.from_numpy(surf_comb[\"v10\"].values[[i - 1, i]][None]),\n",
    "        \"msl\": torch.from_numpy(surf_comb[\"msl\"].values[[i - 1, i]][None]),\n",
    "        \"fire\": torch.from_numpy(surf_comb[\"fire\"].values[[i - 1, i]][None]),\n",
    "        \"lst\": torch.from_numpy(surf_comb[\"lst\"].values[[i - 1, i]][None]),\n",
    "    },\n",
    "    static_vars={\n",
    "        # The static variables are constant, so we just get them for the first time.\n",
    "        \"z\": torch.from_numpy(static_vars_ds[\"z\"].values[0]),\n",
    "        \"slt\": torch.from_numpy(static_vars_ds[\"slt\"].values[0]),\n",
    "        \"lsm\": torch.from_numpy(static_vars_ds[\"lsm\"].values[0]),\n",
    "    },\n",
    "    atmos_vars={\n",
    "        \"t\": torch.from_numpy(atmos_comb[\"t\"].values[[i - 1, i]][None]),\n",
    "        \"u\": torch.from_numpy(atmos_comb[\"u\"].values[[i - 1, i]][None]),\n",
    "        \"v\": torch.from_numpy(atmos_comb[\"v\"].values[[i - 1, i]][None]),\n",
    "        \"q\": torch.from_numpy(atmos_comb[\"q\"].values[[i - 1, i]][None]),\n",
    "        \"z\": torch.from_numpy(atmos_comb[\"z\"].values[[i - 1, i]][None]),\n",
    "    },\n",
    "    metadata=Metadata(\n",
    "        lat=torch.from_numpy(surf_comb.latitude.values),\n",
    "        lon=torch.from_numpy(surf_comb.longitude.values),\n",
    "        # Converting to `datetime64[s]` ensures that the output of `tolist()` gives\n",
    "        # `datetime.datetime`s. Note that this needs to be a tuple of length one:\n",
    "        # one value for every batch element.\n",
    "        time=(surf_comb.valid_time.values.astype(\"datetime64[s]\").tolist()[i],),\n",
    "        atmos_levels=tuple(int(level) for level in atmos_comb.pressure_level.values),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# static_vars_ds = xr.open_dataset(f\"../{download_path}/static/static.nc\", engine=\"netcdf4\")\n",
    "# surf_vars_ds = xr.open_dataset(f\"../{download_path}/fle/surf_2015-01-04.nc\", engine=\"netcdf4\")\n",
    "# atmos_vars_ds = xr.open_dataset(f\"../{download_path}/atmospheric/201501/atmospheric_20150104.nc\", engine=\"netcdf4\")\n",
    "\n",
    "\n",
    "j = i+1  # Select this time index in the downloaded data.\n",
    "\n",
    "surf_size = (720, 1440)\n",
    "atmos_size = (720, 1440)\n",
    "\n",
    "surf_vars={\n",
    "    # First select time points `i` and `i - 1`. Afterwards, `[None]` inserts a\n",
    "    # batch dimension of size one.\n",
    "    \"2t\": torch.from_numpy(surf_comb[\"t2m\"].values[[j]][None]),\n",
    "    \"10u\": torch.from_numpy(surf_comb[\"u10\"].values[[j]][None]),\n",
    "    \"10v\": torch.from_numpy(surf_comb[\"v10\"].values[[j]][None]),\n",
    "    \"msl\": torch.from_numpy(surf_comb[\"msl\"].values[[j]][None]),\n",
    "    \"fire\": torch.from_numpy(surf_comb[\"fire\"].values[[j]][None]),\n",
    "    \"lst\": torch.from_numpy(surf_comb[\"lst\"].values[[j]][None]),\n",
    "}\n",
    "static_vars={\n",
    "    # The static variables are constant, so we just get them for the first time.\n",
    "    \"z\": torch.from_numpy(static_vars_ds[\"z\"].values[0]),\n",
    "    \"slt\": torch.from_numpy(static_vars_ds[\"slt\"].values[0]),\n",
    "    \"lsm\": torch.from_numpy(static_vars_ds[\"lsm\"].values[0]),\n",
    "}\n",
    "atmos_vars={\n",
    "    \"t\": torch.from_numpy(atmos_comb[\"t\"].values[[j]][None]),\n",
    "    \"u\": torch.from_numpy(atmos_comb[\"u\"].values[[j]][None]),\n",
    "    \"v\": torch.from_numpy(atmos_comb[\"v\"].values[[j]][None]),\n",
    "    \"q\": torch.from_numpy(atmos_comb[\"q\"].values[[j]][None]),\n",
    "    \"z\": torch.from_numpy(atmos_comb[\"z\"].values[[j]][None]),\n",
    "}\n",
    "\n",
    "output_size = (720, 1440)\n",
    "\n",
    "interpolated_surf_vars = {\n",
    "    key: F.interpolate(value, size=surf_size, mode='bilinear', align_corners=False)\n",
    "    for key, value in surf_vars.items()\n",
    "}\n",
    "\n",
    "interpolated_atmos_vars = {\n",
    "    key: F.interpolate(value.view(-1, 1, 721, 1440), size=atmos_size, mode='bilinear', align_corners=False).view(value.shape[0], value.shape[1], value.shape[2], 720, 1440)\n",
    "    for key, value in atmos_vars.items()\n",
    "}\n",
    "\n",
    "interpolated_static_vars = {\n",
    "    key: F.interpolate(value.unsqueeze(0).unsqueeze(0), size=output_size, mode='bilinear', align_corners=False).squeeze(0).squeeze(0)\n",
    "    for key, value in static_vars.items()\n",
    "}\n",
    "\n",
    "\n",
    "batch2 = Batch(\n",
    "    surf_vars = interpolated_surf_vars,\n",
    "    static_vars=interpolated_static_vars,\n",
    "    atmos_vars=interpolated_atmos_vars,\n",
    "    metadata=Metadata(\n",
    "        lat=torch.from_numpy(surf_comb.latitude.values),\n",
    "        lon=torch.from_numpy(surf_comb.longitude.values),\n",
    "        # Converting to `datetime64[s]` ensures that the output of `tolist()` gives\n",
    "        # `datetime.datetime`s. Note that this needs to be a tuple of length one:\n",
    "        # one value for every batch element.\n",
    "        time=(surf_comb.valid_time.values.astype(\"datetime64[s]\").tolist()[i],),\n",
    "        atmos_levels=tuple(int(level) for level in atmos_comb.pressure_level.values),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## convert to dataset and dataloader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# using surf_comb is not the best solution\n",
    "# lstdata = torch.from_numpy(surf_comb[\"lst\"].values[[i - 1, i]][None])\n",
    "# Normalisation means:\n",
    "locations[\"fire\"] = 0.0\n",
    "locations[\"lst\"] = 11484.10859\n",
    "\n",
    "# Normalisation standard deviations:\n",
    "scales[\"fire\"] = 1.0\n",
    "scales[\"lst\"] = 8109.33224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def data_retreive ( atmos_files, surf_files, date_range, days, idx , variables_to_keep = None ):\n",
    "    date_window = date_range[idx:idx + days]\n",
    "\n",
    "    atmos_regex = []\n",
    "    surf_regex = []\n",
    "    for day in range(days):\n",
    "        atmos_regex.append( re.compile( rf\".*/{date_window[day].strftime('%Y%m')}/atmospheric_{date_window[day].strftime('%Y%m%d')}\\.nc$\"))\n",
    "        surf_regex.append( re.compile( rf\".*/surf_{date_window[day].strftime('%Y-%m-%d')}\\.nc$\"))\n",
    "\n",
    "    atmos_filt = [f for f in atmos_files if any(d.match(f) for d in atmos_regex)]\n",
    "    surf_filt = [f for f in surf_files if any(d.match(f) for d in surf_regex)]\n",
    "    atmos_list = []\n",
    "    surf_list = []\n",
    "\n",
    "    for day in range(len(atmos_filt)):\n",
    "        with xr.open_dataset(atmos_filt[day])as atmos_cur, xr.open_dataset(surf_filt[day]) as surf_cur :\n",
    "\n",
    "            # code for dropping variables. makes code run slower. interpolation so fast that it doest matter. \n",
    "            if variables_to_keep is not None:\n",
    "                surf_cur = surf_cur.drop_vars([var for var in surf_cur.data_vars if var not in variables_to_keep])\n",
    "\n",
    "            atmos_list.append(atmos_cur.load())\n",
    "            surf_list.append(surf_cur.load())\n",
    "\n",
    "    atmos_comb = xr.concat(atmos_list, dim=\"valid_time\")\n",
    "    surf_comb = xr.concat(surf_list, dim=\"valid_time\")\n",
    "    return atmos_comb, surf_comb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def feature_label_batch(atmos_comb, surf_comb, static, interpolated_static_vars):\n",
    "    i = 1  \n",
    "    batch = Batch(\n",
    "        surf_vars={\n",
    "            # First select time points `i` and `i - 1`. Afterwards, `[None]` inserts a\n",
    "            # batch dimension of size one.\n",
    "            \"2t\": torch.from_numpy(surf_comb[\"t2m\"].values[[i - 1, i]][None]),\n",
    "            \"10u\": torch.from_numpy(surf_comb[\"u10\"].values[[i - 1, i]][None]),\n",
    "            \"10v\": torch.from_numpy(surf_comb[\"v10\"].values[[i - 1, i]][None]),\n",
    "            \"msl\": torch.from_numpy(surf_comb[\"msl\"].values[[i - 1, i]][None]),\n",
    "            \"fire\": torch.from_numpy(surf_comb[\"fire\"].values[[i - 1, i]][None]),\n",
    "            \"lst\": torch.from_numpy(surf_comb[\"lst\"].values[[i - 1, i]][None]),\n",
    "        },\n",
    "        # static_vars={\n",
    "        #     # The static variables are constant, so we just get them for the first time.\n",
    "        #     \"z\": torch.from_numpy(static_vars_ds[\"z\"].values[0]),\n",
    "        #     \"slt\": torch.from_numpy(static_vars_ds[\"slt\"].values[0]),\n",
    "        #     \"lsm\": torch.from_numpy(static_vars_ds[\"lsm\"].values[0]),\n",
    "        # },\n",
    "        static_vars=static ,\n",
    "        atmos_vars={\n",
    "            \"t\": torch.from_numpy(atmos_comb[\"t\"].values[[i - 1, i]][None]),\n",
    "            \"u\": torch.from_numpy(atmos_comb[\"u\"].values[[i - 1, i]][None]),\n",
    "            \"v\": torch.from_numpy(atmos_comb[\"v\"].values[[i - 1, i]][None]),\n",
    "            \"q\": torch.from_numpy(atmos_comb[\"q\"].values[[i - 1, i]][None]),\n",
    "            \"z\": torch.from_numpy(atmos_comb[\"z\"].values[[i - 1, i]][None]),\n",
    "        },\n",
    "        metadata=Metadata(\n",
    "            lat=torch.from_numpy(surf_comb.latitude.values),\n",
    "            lon=torch.from_numpy(surf_comb.longitude.values),\n",
    "            # Converting to `datetime64[s]` ensures that the output of `tolist()` gives\n",
    "            # `datetime.datetime`s. Note that this needs to be a tuple of length one:\n",
    "            # one value for every batch element.\n",
    "            time=(surf_comb.valid_time.values.astype(\"datetime64[s]\").tolist()[i],),\n",
    "            atmos_levels=tuple(int(level) for level in atmos_comb.pressure_level.values),\n",
    "        )\n",
    "        # metadata=metadata\n",
    "    )\n",
    "    \n",
    "    j = 2  # Select this time index in the downloaded data.\n",
    "\n",
    "    surf_size = (720, 1440)\n",
    "    atmos_size = (720, 1440)\n",
    "\n",
    "    surf_vars={\n",
    "        # First select time points `i` and `i - 1`. Afterwards, `[None]` inserts a\n",
    "        # batch dimension of size one.\n",
    "        \"2t\": torch.from_numpy(surf_comb[\"t2m\"].values[[j]][None]),\n",
    "        \"10u\": torch.from_numpy(surf_comb[\"u10\"].values[[j]][None]),\n",
    "        \"10v\": torch.from_numpy(surf_comb[\"v10\"].values[[j]][None]),\n",
    "        \"msl\": torch.from_numpy(surf_comb[\"msl\"].values[[j]][None]),\n",
    "        \"fire\": torch.from_numpy(surf_comb[\"fire\"].values[[j]][None]),\n",
    "        # ************** Comment out LST\n",
    "        \"lst\": torch.from_numpy(surf_comb[\"lst\"].values[[j]][None]),\n",
    "    }\n",
    "    # static_vars={\n",
    "    #     # The static variables are constant, so we just get them for the first time.\n",
    "    #     \"z\": torch.from_numpy(static_vars_ds[\"z\"].values[0]),\n",
    "    #     \"slt\": torch.from_numpy(static_vars_ds[\"slt\"].values[0]),\n",
    "    #     \"lsm\": torch.from_numpy(static_vars_ds[\"lsm\"].values[0]),\n",
    "    # }\n",
    "    # static_vars=static \n",
    "    atmos_vars={\n",
    "        \"t\": torch.from_numpy(atmos_comb[\"t\"].values[[j]][None]),\n",
    "        \"u\": torch.from_numpy(atmos_comb[\"u\"].values[[j]][None]),\n",
    "        \"v\": torch.from_numpy(atmos_comb[\"v\"].values[[j]][None]),\n",
    "        \"q\": torch.from_numpy(atmos_comb[\"q\"].values[[j]][None]),\n",
    "        \"z\": torch.from_numpy(atmos_comb[\"z\"].values[[j]][None]),\n",
    "    }\n",
    "\n",
    "    output_size = (720, 1440)\n",
    "\n",
    "    interpolated_surf_vars = {\n",
    "        key: F.interpolate(value, size=surf_size, mode='bilinear', align_corners=False)\n",
    "        for key, value in surf_vars.items()\n",
    "    }\n",
    "\n",
    "    interpolated_atmos_vars = {\n",
    "        key: F.interpolate(value.view(-1, 1, 721, 1440), size=atmos_size, mode='bilinear', align_corners=False).view(value.shape[0], value.shape[1], value.shape[2], 720, 1440)\n",
    "        for key, value in atmos_vars.items()\n",
    "    }\n",
    "\n",
    "    # interpolated_static_vars = {\n",
    "    #     key: F.interpolate(value.unsqueeze(0).unsqueeze(0), size=output_size, mode='bilinear', align_corners=False).squeeze(0).squeeze(0)\n",
    "    #     for key, value in static_vars.items()\n",
    "    # }\n",
    "\n",
    "\n",
    "    batch2 = Batch(\n",
    "        surf_vars = interpolated_surf_vars,\n",
    "        static_vars=interpolated_static_vars,\n",
    "        atmos_vars=interpolated_atmos_vars,\n",
    "        metadata=Metadata(\n",
    "            lat=torch.from_numpy(surf_comb.latitude.values),\n",
    "            lon=torch.from_numpy(surf_comb.longitude.values),\n",
    "            # Converting to `datetime64[s]` ensures that the output of `tolist()` gives\n",
    "            # `datetime.datetime`s. Note that this needs to be a tuple of length one:\n",
    "            # one value for every batch element.\n",
    "            time=(surf_comb.valid_time.values.astype(\"datetime64[s]\").tolist()[j],),\n",
    "            atmos_levels=tuple(int(level) for level in atmos_comb.pressure_level.values),\n",
    "        ),\n",
    "        # metadata=metadata\n",
    "    )\n",
    "    return batch, batch2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AuroraDataset(Dataset):\n",
    "    def __init__(self, download_path, variables_to_keep , starty, endy):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            folder_path (str): Path to the folder containing the daily files.\n",
    "            transform (callable, optional): A function/transform to apply to the data.\n",
    "        \"\"\"\n",
    "        with xr.open_dataset(f\"../{download_path}/static/static.nc\", engine=\"netcdf4\") as static_vars_ds:\n",
    "            self.static={\n",
    "                # The static variables are constant, so we just get them for the first time.\n",
    "                \"z\": torch.from_numpy(static_vars_ds[\"z\"].values[0]),\n",
    "                \"slt\": torch.from_numpy(static_vars_ds[\"slt\"].values[0]),\n",
    "                \"lsm\": torch.from_numpy(static_vars_ds[\"lsm\"].values[0]),\n",
    "            }\n",
    "            self.interp_static = {\n",
    "                key: F.interpolate(value.unsqueeze(0).unsqueeze(0), size=(720, 1440), mode='bilinear', align_corners=False).squeeze(0).squeeze(0)\n",
    "                for key, value in self.static.items()\n",
    "            }\n",
    "        self.atmos_files = glob.glob(f\"../{download_path}/atmospheric/*/*.nc\") \n",
    "        self.surf_files = glob.glob(f\"../{download_path}/fle/*.nc\") \n",
    "        \n",
    "        self.window = 3\n",
    "        self.variables_to_keep = variables_to_keep\n",
    "        \n",
    "        self.date_range = pd.date_range(start=f'{starty}-01-01', end=f'{endy}-12-31', freq='D')\n",
    "        \n",
    "        # self.file_names = sorted(os.listdir(folder_path))  # Sort to ensure correct order\n",
    "        # self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the total number of files (days).\"\"\"\n",
    "        return len(self.surf_files) - self.window + 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            idx (int): Index of the file to load.\n",
    "        Returns:\n",
    "            (Tensor, Tensor): Features and labels for the given day.\n",
    "        \"\"\"\n",
    "#         file_path = os.path.join(self.folder_path, self.file_names[idx])\n",
    "        \n",
    "        #                       \n",
    "        atmos, surf = data_retreive(self.atmos_files, self.surf_files, self.date_range\n",
    "                             , self.window, idx, self.variables_to_keep)\n",
    "        \n",
    "        # metadata=Metadata(\n",
    "        #     lat=torch.from_numpy(surf.latitude.values),\n",
    "        #     lon=torch.from_numpy(surf.longitude.values),\n",
    "        #     # Converting to `datetime64[s]` ensures that the output of `tolist()` gives\n",
    "        #     # `datetime.datetime`s. Note that this needs to be a tuple of length one:\n",
    "        #     # one value for every batch element.\n",
    "        #     time=(surf.valid_time.values.astype(\"datetime64[s]\").tolist()[1],),\n",
    "        #     atmos_levels=tuple(int(level) for level in atmos.pressure_level.values),\n",
    "        # )\n",
    "        # Assuming the last column is the label\n",
    "        features, labels = feature_label_batch(atmos, surf, self.static, self.interp_static)\n",
    "         # = label_batch(atmos, surf, self.interp_static, metadata) # metadata technically wrong for labels\n",
    "\n",
    "        # Convert to tensors\n",
    "        return (\n",
    "            features, labels\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def aurora_collate(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function for handling Batch objects as features and labels.\n",
    "\n",
    "    Args:\n",
    "        batch (list of tuples): Each tuple contains (features, labels), where\n",
    "                                features and labels are instances of `Batch`.\n",
    "\n",
    "    Returns:\n",
    "        Batched features and labels.\n",
    "    \"\"\"\n",
    "    # Separate features and labels from the batch\n",
    "    features_list, labels_list = zip(*batch)\n",
    "\n",
    "    # Function to batch the variables in each component of the `Batch` object\n",
    "    def collate_batch_components(batch_list, key):\n",
    "        return {\n",
    "            var_key: torch.stack([getattr(b, key)[var_key] for b in batch_list])\n",
    "            for var_key in getattr(batch_list[0], key)}\n",
    "\n",
    "    # Combine `surf_vars` and `atmos_vars` for features and labels\n",
    "    batched_features = Batch(\n",
    "        surf_vars=collate_batch_components(features_list, \"surf_vars\"),\n",
    "        static_vars=collate_batch_components(features_list, \"static_vars\"),\n",
    "        atmos_vars=collate_batch_components(features_list, \"atmos_vars\"),\n",
    "        metadata=Metadata(\n",
    "            lat=torch.stack([f.metadata.lat for f in features_list]),\n",
    "            lon=torch.stack([f.metadata.lon for f in features_list]),\n",
    "            time=[f.metadata.time for f in features_list],\n",
    "            atmos_levels=[f.metadata.atmos_levels for f in features_list],\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    batched_labels = Batch(\n",
    "        surf_vars=collate_batch_components(labels_list, \"surf_vars\"),\n",
    "        static_vars=collate_batch_components(features_list, \"static_vars\"),\n",
    "        atmos_vars=collate_batch_components(labels_list, \"atmos_vars\"),\n",
    "        metadata=Metadata(\n",
    "            lat=torch.stack([l.metadata.lat for l in labels_list]),\n",
    "            lon=torch.stack([l.metadata.lon for l in labels_list]),\n",
    "            time=[l.metadata.time for l in labels_list],\n",
    "            atmos_levels=[l.metadata.atmos_levels for l in labels_list],\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    return features_list[0], labels_list[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run as single block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalisation means:\n",
    "# locations[\"fire\"] = 0.0\n",
    "# locations[\"lst\"] = 11484.10859\n",
    "\n",
    "# Normalisation standard deviations:\n",
    "# scales[\"fire\"] = 1.0\n",
    "# scales[\"lst\"] = 8109.33224\n",
    "\n",
    "# variables_to_keep = ['u10', 'v10', 't2m', 'msl', 'fire']\n",
    "# variables_to_keep = ['u10', 'v10', 't2m', 'msl', 'lst', 'fire']\n",
    "\n",
    "def data_retreive ( atmos_files, surf_files, date_range, days, idx , variables_to_keep = None ):\n",
    "    date_window = date_range[idx:idx + days]\n",
    "    # print(date_range)\n",
    "    # print(idx)\n",
    "    # print(days)\n",
    "    atmos_regex = []\n",
    "    surf_regex = []\n",
    "    for day in range(days):\n",
    "        # note that no need to look at download path here because its already accounted for when globing inside of aurora dataset\n",
    "        # atmos_regex.append( re.compile( rf\"\\.\\./{download_path}/atmospheric/{date_window[day].strftime('%Y%m')}/atmospheric_{date_window[day].strftime('%Y%m%d')}\\.nc$\"))\n",
    "        # surf_regex.append( re.compile( rf\"\\.\\./{download_path}/fle/surf_{date_window[day].strftime('%Y-%m-%d')}\\.nc$\"))\n",
    "        # atmos_regex.append( re.compile( rf\".*/{download_path}.*/{date_window[day].strftime('%Y%m')}/atmospheric_{date_window[day].strftime('%Y%m%d')}\\.nc$\"))\n",
    "        # surf_regex.append( re.compile( rf\".*/{download_path}.*/surf_{date_window[day].strftime('%Y-%m-%d')}\\.nc$\"))\n",
    "        atmos_regex.append( re.compile( rf\".*/{date_window[day].strftime('%Y%m')}/atmospheric_{date_window[day].strftime('%Y%m%d')}\\.nc$\"))\n",
    "        surf_regex.append( re.compile( rf\".*/surf_{date_window[day].strftime('%Y-%m-%d')}\\.nc$\"))\n",
    "\n",
    "    atmos_filt = [f for f in atmos_files if any(d.match(f) for d in atmos_regex)]\n",
    "    surf_filt = [f for f in surf_files if any(d.match(f) for d in surf_regex)]\n",
    "    atmos_list = []\n",
    "    surf_list = []\n",
    "    # print(atmos_filt)\n",
    "    # print(\"surf\")\n",
    "    # print(surf_filt)\n",
    "    ds = xr.open_dataset(atmos_filt[0])\n",
    "    ds.close()\n",
    "\n",
    "    # for day in range(len(atmos_filt)):\n",
    "    for day in range(days):\n",
    "        # surf_cur = None  # Initialize to None\n",
    "        # atmos_cur = None  # Initialize to None\n",
    "        # print(atmos_filt[day])\n",
    "        # with xr.open_dataset(atmos_filt[day])as atmos_cur, xr.open_dataset(surf_filt[day]) as surf_cur :\n",
    "        try: \n",
    "        # with xr.open_dataset(atmos_filt[day])as atmos_cur, xr.open_dataset(surf_filt[day]) as surf_cur :\n",
    "        # with xr.open_dataset(atmos_filt[day])as atmos_cur, xr.open_dataset(surf_filt[day]) as surf_cur :\n",
    "            atmos_cur = xr.open_dataset(atmos_filt[day])\n",
    "            surf_cur = xr.open_dataset(surf_filt[day])\n",
    "        \n",
    "\n",
    "            # code for dropping variables. makes code run slower. interpolation so fast that it doest matter. \n",
    "            if variables_to_keep is not None:\n",
    "                surf_cur = surf_cur.drop_vars([var for var in surf_cur.data_vars if var not in variables_to_keep])\n",
    "\n",
    "            atmos_list.append(atmos_cur.load())\n",
    "            surf_list.append(surf_cur.load())\n",
    "            \n",
    "        finally: \n",
    "            if atmos_cur is not None:\n",
    "                atmos_cur.close()\n",
    "            if surf_cur is not None:\n",
    "                surf_cur.close()\n",
    "\n",
    "    atmos_comb = xr.concat(atmos_list, dim=\"valid_time\")\n",
    "    surf_comb = xr.concat(surf_list, dim=\"valid_time\")\n",
    "    return atmos_comb, surf_comb\n",
    "\n",
    "def feature_label_batch(atmos_comb, surf_comb, static, interpolated_static_vars):\n",
    "    i = 1  \n",
    "    batch = Batch(\n",
    "        surf_vars={\n",
    "            # First select time points `i` and `i - 1`. Afterwards, `[None]` inserts a\n",
    "            # batch dimension of size one.\n",
    "            \"2t\": torch.from_numpy(surf_comb[\"t2m\"].values[[i - 1, i]][None]),\n",
    "            \"10u\": torch.from_numpy(surf_comb[\"u10\"].values[[i - 1, i]][None]),\n",
    "            \"10v\": torch.from_numpy(surf_comb[\"v10\"].values[[i - 1, i]][None]),\n",
    "            \"msl\": torch.from_numpy(surf_comb[\"msl\"].values[[i - 1, i]][None]),\n",
    "            \"fire\": torch.from_numpy(surf_comb[\"fire\"].values[[i - 1, i]][None]),\n",
    "            \"lst\": torch.from_numpy(surf_comb[\"lst\"].values[[i - 1, i]][None]),\n",
    "        },\n",
    "        # static_vars={\n",
    "        #     # The static variables are constant, so we just get them for the first time.\n",
    "        #     \"z\": torch.from_numpy(static_vars_ds[\"z\"].values[0]),\n",
    "        #     \"slt\": torch.from_numpy(static_vars_ds[\"slt\"].values[0]),\n",
    "        #     \"lsm\": torch.from_numpy(static_vars_ds[\"lsm\"].values[0]),\n",
    "        # },\n",
    "        static_vars=static ,\n",
    "        atmos_vars={\n",
    "            \"t\": torch.from_numpy(atmos_comb[\"t\"].values[[i - 1, i]][None]),\n",
    "            \"u\": torch.from_numpy(atmos_comb[\"u\"].values[[i - 1, i]][None]),\n",
    "            \"v\": torch.from_numpy(atmos_comb[\"v\"].values[[i - 1, i]][None]),\n",
    "            \"q\": torch.from_numpy(atmos_comb[\"q\"].values[[i - 1, i]][None]),\n",
    "            \"z\": torch.from_numpy(atmos_comb[\"z\"].values[[i - 1, i]][None]),\n",
    "        },\n",
    "        metadata=Metadata(\n",
    "            lat=torch.from_numpy(surf_comb.latitude.values),\n",
    "            lon=torch.from_numpy(surf_comb.longitude.values),\n",
    "            # Converting to `datetime64[s]` ensures that the output of `tolist()` gives\n",
    "            # `datetime.datetime`s. Note that this needs to be a tuple of length one:\n",
    "            # one value for every batch element.\n",
    "            time=(surf_comb.valid_time.values.astype(\"datetime64[s]\").tolist()[i],),\n",
    "            atmos_levels=tuple(int(level) for level in atmos_comb.pressure_level.values),\n",
    "        )\n",
    "        # metadata=metadata\n",
    "    )\n",
    "    \n",
    "    print(np.unique(batch.surf_vars[\"fire\"].cpu().numpy()))\n",
    "    \n",
    "    j = 2  # Select this time index in the downloaded data.\n",
    "\n",
    "    surf_size = (720, 1440)\n",
    "    atmos_size = (720, 1440)\n",
    "\n",
    "    surf_vars={\n",
    "        # First select time points `i` and `i - 1`. Afterwards, `[None]` inserts a\n",
    "        # batch dimension of size one.\n",
    "        \"2t\": torch.from_numpy(surf_comb[\"t2m\"].values[[j]][None]),\n",
    "        \"10u\": torch.from_numpy(surf_comb[\"u10\"].values[[j]][None]),\n",
    "        \"10v\": torch.from_numpy(surf_comb[\"v10\"].values[[j]][None]),\n",
    "        \"msl\": torch.from_numpy(surf_comb[\"msl\"].values[[j]][None]),\n",
    "        \"fire\": torch.from_numpy(surf_comb[\"fire\"].values[[j]][None]),\n",
    "        # ************** Comment out LST\n",
    "        \"lst\": torch.from_numpy(surf_comb[\"lst\"].values[[j]][None]),\n",
    "    }\n",
    "    print(np.unique(surf_vars[\"fire\"].cpu().numpy()))\n",
    "    atmos_vars={\n",
    "        \"t\": torch.from_numpy(atmos_comb[\"t\"].values[[j]][None]),\n",
    "        \"u\": torch.from_numpy(atmos_comb[\"u\"].values[[j]][None]),\n",
    "        \"v\": torch.from_numpy(atmos_comb[\"v\"].values[[j]][None]),\n",
    "        \"q\": torch.from_numpy(atmos_comb[\"q\"].values[[j]][None]),\n",
    "        \"z\": torch.from_numpy(atmos_comb[\"z\"].values[[j]][None]),\n",
    "    }\n",
    "\n",
    "    output_size = (720, 1440)\n",
    "\n",
    "\n",
    "    # interpolated_surf_vars = {\n",
    "    #     key: (F.interpolate(value, size=surf_size, mode='bilinear', align_corners=False) >= 0.5).int()\n",
    "    #     if key == \"fire\" else  F.interpolate(value, size=surf_size, mode='bilinear', align_corners=False)\n",
    "    #     for key, value in surf_vars.items()\n",
    "    # }\n",
    "\n",
    "    interpolated_surf_vars = {\n",
    "        key: F.interpolate(value, size=surf_size, mode='bilinear', align_corners=False)\n",
    "        for key, value in surf_vars.items()\n",
    "    }\n",
    "\n",
    "    print(\"before count\", surf_vars[\"fire\"].count_nonzero(y == 1) )\n",
    "    print(\"after count\", interpolated_surf_vars[\"fire\"].count_nonzero(y == 1) )\n",
    "    \n",
    "    interpolated_atmos_vars = {\n",
    "        key: F.interpolate(value.view(-1, 1, 721, 1440), size=atmos_size, mode='bilinear', align_corners=False).view(value.shape[0], value.shape[1], value.shape[2], 720, 1440)\n",
    "        for key, value in atmos_vars.items()\n",
    "    }\n",
    "\n",
    "    batch2 = Batch(\n",
    "        surf_vars = interpolated_surf_vars,\n",
    "        static_vars=interpolated_static_vars,\n",
    "        atmos_vars=interpolated_atmos_vars,\n",
    "        metadata=Metadata(\n",
    "            lat=torch.from_numpy(surf_comb.latitude.values),\n",
    "            lon=torch.from_numpy(surf_comb.longitude.values),\n",
    "            # Converting to `datetime64[s]` ensures that the output of `tolist()` gives\n",
    "            # `datetime.datetime`s. Note that this needs to be a tuple of length one:\n",
    "            # one value for every batch element.\n",
    "            time=(surf_comb.valid_time.values.astype(\"datetime64[s]\").tolist()[j],),\n",
    "            atmos_levels=tuple(int(level) for level in atmos_comb.pressure_level.values),\n",
    "        ),\n",
    "        # metadata=metadata\n",
    "    )\n",
    "    return batch, batch2\n",
    "\n",
    "class AuroraDataset(Dataset):\n",
    "    def __init__(self, download_path, variables_to_keep , date_range):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            folder_path (str): Path to the folder containing the daily files.\n",
    "            transform (callable, optional): A function/transform to apply to the data.\n",
    "        \"\"\"\n",
    "        self.sumides = 5\n",
    "        with xr.open_dataset(f\"../{download_path}/static/static.nc\", engine=\"netcdf4\") as static_vars_ds :\n",
    "            # static_vars_ds = pstatic_vars_ds\n",
    "            \n",
    "            prestatic={\n",
    "                # The static variables are constant, so we just get them for the first time.\n",
    "                \"z\": torch.from_numpy(np.copy(static_vars_ds[\"z\"].values[0])),\n",
    "                \"slt\": torch.from_numpy(np.copy(static_vars_ds[\"slt\"].values[0])),\n",
    "                \"lsm\": torch.from_numpy(np.copy(static_vars_ds[\"lsm\"].values[0])),\n",
    "            }\n",
    "            # prestatic={\n",
    "            #     # The static variables are constant, so we just get them for the first time.\n",
    "            #     \"z\": torch.from_numpy(static_vars_ds[\"z\"].values[0].copy()),\n",
    "            #     \"slt\": torch.from_numpy(static_vars_ds[\"slt\"].values[0].copy()),\n",
    "            #     \"lsm\": torch.from_numpy(static_vars_ds[\"lsm\"].values[0].copy()),\n",
    "            # }\n",
    "            print(prestatic)\n",
    "            self.static= prestatic\n",
    "            self.interp_static = {\n",
    "                key: F.interpolate(value.unsqueeze(0).unsqueeze(0), size=(720, 1440), mode='bilinear', align_corners=False).squeeze(0).squeeze(0)\n",
    "                for key, value in prestatic.items()\n",
    "            }\n",
    "\n",
    "        # static_vars_ds = xr.open_dataset(f\"../{download_path}/static/static.nc\", engine=\"netcdf4\")\n",
    "        # static={\n",
    "        #         # The static variables are constant, so we just get them for the first time.\n",
    "        #         \"z\": torch.from_numpy(static_vars_ds[\"z\"].values[0]),\n",
    "        #         \"slt\": torch.from_numpy(static_vars_ds[\"slt\"].values[0]),\n",
    "        #         \"lsm\": torch.from_numpy(static_vars_ds[\"lsm\"].values[0]),\n",
    "        #     }\n",
    "        # self.interp_static = {\n",
    "        #         key: F.interpolate(value.unsqueeze(0).unsqueeze(0), size=(720, 1440), mode='bilinear', align_corners=False).squeeze(0).squeeze(0)\n",
    "        #         for key, value in static.items()\n",
    "        #     }\n",
    "        # static_vars_ds.close()\n",
    "\n",
    "        \n",
    "        # with xr.open_dataset(f\"../{download_path}/static/static.nc\", engine=\"netcdf4\") as static_vars_ds:\n",
    "        #     self.static={\n",
    "        #         # The static variables are constant, so we just get them for the first time.\n",
    "        #         \"z\": torch.from_numpy(static_vars_ds[\"z\"].values[0]),\n",
    "        #         \"slt\": torch.from_numpy(static_vars_ds[\"slt\"].values[0]),\n",
    "        #         \"lsm\": torch.from_numpy(static_vars_ds[\"lsm\"].values[0]),\n",
    "        #     }\n",
    "        #     self.interp_static = {\n",
    "        #         key: F.interpolate(value.unsqueeze(0).unsqueeze(0), size=(720, 1440), mode='bilinear', align_corners=False).squeeze(0).squeeze(0)\n",
    "        #         for key, value in self.static.items()\n",
    "        #     }\n",
    "        # self.atmos_files = glob.glob(f\"../{download_path}/atmospheric/*/*.nc\") \n",
    "        # self.surf_files = glob.glob(f\"../{download_path}/fle/*.nc\") \n",
    "        \n",
    "        self.window = 3\n",
    "        self.variables_to_keep = variables_to_keep\n",
    "        \n",
    "        self.date_range = date_range\n",
    "        self.download_path = download_path\n",
    "        \n",
    "        # self.file_names = sorted(os.listdir(folder_path))  # Sort to ensure correct order\n",
    "        # self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the total number of files (days).\"\"\"\n",
    "        # return len(self.surf_files) - self.window + 1\n",
    "        return len(date_range) - self.window + 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            idx (int): Index of the file to load.\n",
    "        Returns:\n",
    "            (Tensor, Tensor): Features and labels for the given day.\n",
    "        \"\"\"\n",
    "#         file_path = os.path.join(self.folder_path, self.file_names[idx])\n",
    "        \n",
    "        #                       \n",
    "        atmos, surf = data_retreive(self.atmos_files, self.surf_files, self.date_range\n",
    "                             , self.window, idx,  self.variables_to_keep)\n",
    "        \n",
    "        # metadata=Metadata(\n",
    "        #     lat=torch.from_numpy(surf.latitude.values),\n",
    "        #     lon=torch.from_numpy(surf.longitude.values),\n",
    "        #     # Converting to `datetime64[s]` ensures that the output of `tolist()` gives\n",
    "        #     # `datetime.datetime`s. Note that this needs to be a tuple of length one:\n",
    "        #     # one value for every batch element.\n",
    "        #     time=(surf.valid_time.values.astype(\"datetime64[s]\").tolist()[1],),\n",
    "        #     atmos_levels=tuple(int(level) for level in atmos.pressure_level.values),\n",
    "        # )\n",
    "        # Assuming the last column is the label\n",
    "        features, labels = feature_label_batch(atmos, surf, self.static, self.interp_static)\n",
    "         # = label_batch(atmos, surf, self.interp_static, metadata) # metadata technically wrong for labels\n",
    "        \n",
    "        print(\"in getitem feature:\", np.unique(features.surf_vars[\"fire\"].cpu().numpy()))\n",
    "        print(\"in getitem label:\", np.unique(labels.surf_vars[\"fire\"].cpu().numpy()))\n",
    "        # Convert to tensors\n",
    "        return (\n",
    "            features, labels\n",
    "        )\n",
    "\n",
    "def aurora_collate(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function for handling Batch objects as features and labels.\n",
    "\n",
    "    Args:\n",
    "        batch (list of tuples): Each tuple contains (features, labels), where\n",
    "                                features and labels are instances of `Batch`.\n",
    "\n",
    "    Returns:\n",
    "        Batched features and labels.\n",
    "    \"\"\"\n",
    "    # Separate features and labels from the batch\n",
    "    features_list, labels_list = zip(*batch)\n",
    "\n",
    "    # Function to batch the variables in each component of the `Batch` object\n",
    "    def collate_batch_components(batch_list, key):\n",
    "        return {\n",
    "            var_key: torch.stack([getattr(b, key)[var_key] for b in batch_list])\n",
    "            for var_key in getattr(batch_list[0], key)}\n",
    "\n",
    "    # Combine `surf_vars` and `atmos_vars` for features and labels\n",
    "    batched_features = Batch(\n",
    "        surf_vars=collate_batch_components(features_list, \"surf_vars\"),\n",
    "        static_vars=collate_batch_components(features_list, \"static_vars\"),\n",
    "        atmos_vars=collate_batch_components(features_list, \"atmos_vars\"),\n",
    "        metadata=Metadata(\n",
    "            lat=torch.stack([f.metadata.lat for f in features_list]),\n",
    "            lon=torch.stack([f.metadata.lon for f in features_list]),\n",
    "            time=[f.metadata.time for f in features_list],\n",
    "            atmos_levels=[f.metadata.atmos_levels for f in features_list],\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    batched_labels = Batch(\n",
    "        surf_vars=collate_batch_components(labels_list, \"surf_vars\"),\n",
    "        static_vars=collate_batch_components(features_list, \"static_vars\"),\n",
    "        atmos_vars=collate_batch_components(labels_list, \"atmos_vars\"),\n",
    "        metadata=Metadata(\n",
    "            lat=torch.stack([l.metadata.lat for l in labels_list]),\n",
    "            lon=torch.stack([l.metadata.lon for l in labels_list]),\n",
    "            time=[l.metadata.time for l in labels_list],\n",
    "            atmos_levels=[l.metadata.atmos_levels for l in labels_list],\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    return features_list[0], labels_list[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## dataset/loader testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "AuroraDataset.__init__() takes 4 positional arguments but 5 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3984/1435884618.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAuroraDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdownload_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariables_to_keep\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mstarty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maurora_collate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: AuroraDataset.__init__() takes 4 positional arguments but 5 were given"
     ]
    }
   ],
   "source": [
    "starty = 2015     ## start of 2015 to end of 2021\n",
    "endy = 2021           \n",
    "date_range = pd.date_range(start=f'{starty}-01-01', end=f'{endy}-12-31', freq='D')\n",
    "variables_to_keep = ['u10', 'v10', 't2m', 'msl', 'lst', 'fire']\n",
    "\n",
    "download_path = \"datasets/aurora\"\n",
    "\n",
    "\n",
    "dataset = AuroraDataset(download_path, variables_to_keep , starty, endy)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True, collate_fn=aurora_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for features, labels in dataloader:\n",
    "    print(features.surf_vars[\"2t\"].shape)  # Ensure shapes are correct\n",
    "    print(labels.surf_vars[\"2t\"].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for batch_data, batch_labels in dataloader:\n",
    "    print(batch_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing cell\n",
    "\n",
    "print(batch.surf_vars['2t'].size())\n",
    "print(interpolated_surf_vars['2t'].size())\n",
    "print(interpolated_atmos_vars['t'].size())\n",
    "\n",
    "# print(.size())\n",
    "# print(f\" size: {.size()}\")\n",
    "print(atmos_vars['t'].view(-1, 1, 721, 1440).size())\n",
    "print(atmos_vars['t'].view(-1, 1, 721, 1440).shape[2])\n",
    "\n",
    "# interpolated_surf_vars\n",
    "# surf_vars\n",
    "\n",
    "# print(atmos_vars['t'].size())\n",
    "# print(atmos_vars['u'].size())\n",
    "# print(atmos_vars['v'].size())\n",
    "# print(atmos_vars['q'].size())\n",
    "# print(atmos_vars['z'].size())\n",
    "# torch.from_numpy(surf_comb[\"t2m\"].values[[i - 1, i]][None])\n",
    "\n",
    "print(static_vars['z'].unsqueeze(0).unsqueeze(0).squeeze(0).size())\n",
    "\n",
    "# atmos_vars.items()\n",
    "print(batch2.surf_vars['2t'].size())\n",
    "print(interpolated_surf_vars['2t'].size())\n",
    "batch2.surf_vars['2t']\n",
    "\n",
    "print(f\"batch2 size: {batch2.surf_vars['2t'].size()}\")\n",
    "print(f\"pred size: {pred.surf_vars['2t'].size()}\")\n",
    "batch2.surf_vars['2t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpolated_surf_vars['2t']\n",
    "# pred.surf_vars['2t']\n",
    "batch.surf_vars['2t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test =  torch.from_numpy(surf_vars_ds[\"lst\"].values[[i - 1, i]][None])\n",
    "test.mean()\n",
    "test.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "\n",
    "def loss_func(batch, pred):\n",
    "    crit = nn.L1Loss()\n",
    "    batch = batch.to('cuda')\n",
    "    pred = pred.to('cuda')\n",
    "    v_w = {}\n",
    "    alpha = 0.25\n",
    "    beta = 1\n",
    "    dataset_weight = 2\n",
    "    total_vars = len(batch.surf_vars) + len(batch.atmos_vars)\n",
    "    v_w[\"2t\"] = 3.0\n",
    "    v_w[\"10u\"] = 0.77\n",
    "    v_w[\"10v\"] = 0.66\n",
    "    v_w[\"msl\"] = 1.5\n",
    "    v_w[\"z\"] = 2.8\n",
    "    v_w[\"q\"] = 0.78\n",
    "    v_w[\"t\"] = 1.7\n",
    "    v_w[\"u\"] = 0.87\n",
    "    v_w[\"v\"] = 0.6\n",
    "    v_w[\"fire\"] = 1\n",
    "    # ******* Comment out LST for now \n",
    "    # no need to comment them out as below code only checks for existing ones\n",
    "    v_w[\"lst\"] = 1\n",
    "    l_s = torch.zeros(1, dtype=torch.float32, device=device)\n",
    "    l_a = torch.zeros(1, dtype=torch.float32, device=device)\n",
    "    for key in batch.surf_vars.keys():\n",
    "        l_s += v_w[key]*crit(pred.surf_vars[key] + 1e-8, batch.surf_vars[key] + 1e-8)/(721*1440)\n",
    "    for key in batch.atmos_vars.keys():\n",
    "        l_a += v_w[key]*crit(pred.atmos_vars[key] + 1e-8, batch.atmos_vars[key] + 1e-8)/(721*1440*13)\n",
    "    \n",
    "    l = (2/total_vars)*(l_s * alpha) + (l_a*beta)\n",
    "\n",
    "    return l   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del locations[\"fire\"]\n",
    "# del scales[\"fire\"]\n",
    "# locations\n",
    "# print(locations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "when changing things, change out instances of it in 5, location/scales, model itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'z': tensor([[ 3.2275e-01,  3.2275e-01,  3.2275e-01,  ...,  3.2275e-01,\n",
      "          3.2275e-01,  3.2275e-01],\n",
      "        [ 4.6157e+00,  4.6040e+00,  4.5884e+00,  ...,  4.7915e+00,\n",
      "          4.7329e+00,  4.6743e+00],\n",
      "        [-1.7710e+00, -1.7163e+00, -1.7085e+00,  ..., -1.5093e+00,\n",
      "         -1.6226e+00, -1.7124e+00],\n",
      "        ...,\n",
      "        [ 2.6958e+04,  2.6961e+04,  2.6965e+04,  ...,  2.6950e+04,\n",
      "          2.6953e+04,  2.6956e+04],\n",
      "        [ 2.7163e+04,  2.7165e+04,  2.7167e+04,  ...,  2.7159e+04,\n",
      "          2.7160e+04,  2.7162e+04],\n",
      "        [ 2.7355e+04,  2.7355e+04,  2.7355e+04,  ...,  2.7355e+04,\n",
      "          2.7355e+04,  2.7355e+04]]), 'slt': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.]]), 'lsm': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.]])}\n"
     ]
    }
   ],
   "source": [
    "# Normalisation means:\n",
    "locations[\"fire\"] =1e-8\n",
    "locations[\"lst\"] = 11484.10859\n",
    "\n",
    "# Normalisation standard deviations:\n",
    "scales[\"fire\"] = 1.0\n",
    "scales[\"lst\"] = 8109.33224\n",
    "starty = 2015     ## start of 2015 to end of 2021\n",
    "endy = 2015           \n",
    "date_range = pd.date_range(start=f'{starty}-01-01', end=f'{endy}-01-05', freq='D')\n",
    "date_range = pd.date_range(start=f'{starty}-01-01', end=f'{endy}-12-31', freq='D')\n",
    "# if keyerror, dleete variable\n",
    "# variables_to_keep = ['u10', 'v10', 't2m', 'msl', 'fire']\n",
    "# no need to worry about dong less. manually managug from 5 is way to edit for now\n",
    "variables_to_keep = ['u10', 'v10', 't2m', 'msl', 'lst', 'fire']\n",
    "\n",
    "download_path = \"globfire-gooddata/data\"\n",
    "# download_path = \"globfire-gooddata\"\n",
    "\n",
    "dataset = AuroraDataset(download_path, variables_to_keep , date_range)\n",
    "\n",
    "train_size = int(0.8 * len(dataset))  \n",
    "train_size = int(0.95 * len(dataset))  \n",
    "test_size = len(dataset) - train_size \n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True, collate_fn=aurora_collate)\n",
    "dataloader_test = DataLoader(test_dataset, batch_size=1, shuffle=True, collate_fn=aurora_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xr.set_options(file_cache_maxsize=128)\n",
    "\n",
    "# for batch in dataloader:\n",
    "#     print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'z': tensor([[ 3.2275e-01,  3.2275e-01,  3.2275e-01,  ...,  3.2275e-01,\n",
      "          3.2275e-01,  3.2275e-01],\n",
      "        [ 4.6157e+00,  4.6040e+00,  4.5884e+00,  ...,  4.7915e+00,\n",
      "          4.7329e+00,  4.6743e+00],\n",
      "        [-1.7710e+00, -1.7163e+00, -1.7085e+00,  ..., -1.5093e+00,\n",
      "         -1.6226e+00, -1.7124e+00],\n",
      "        ...,\n",
      "        [ 2.6958e+04,  2.6961e+04,  2.6965e+04,  ...,  2.6950e+04,\n",
      "          2.6953e+04,  2.6956e+04],\n",
      "        [ 2.7163e+04,  2.7165e+04,  2.7167e+04,  ...,  2.7159e+04,\n",
      "          2.7160e+04,  2.7162e+04],\n",
      "        [ 2.7355e+04,  2.7355e+04,  2.7355e+04,  ...,  2.7355e+04,\n",
      "          2.7355e+04,  2.7355e+04]]), 'slt': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.]]), 'lsm': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.]])}\n"
     ]
    }
   ],
   "source": [
    "# import netCDF4 as nc\n",
    "# try:\n",
    "#     ds = nc.Dataset('/home/ubuntu/globfire-gooddata/data/atmospheric/201503/atmospheric_20150302.nc')\n",
    "#     print(ds)\n",
    "# except Exception as e:\n",
    "#     print(f\"Error: {e}\")\n",
    "\n",
    "asdf = xr.open_dataset(f\"../{download_path}/static/static.nc\", engine=\"netcdf4\")\n",
    "asdf2={\n",
    "                # The static variables are constant, so we just get them for the first time.\n",
    "                \"z\": torch.from_numpy(asdf[\"z\"].values[0]),\n",
    "                \"slt\": torch.from_numpy(asdf[\"slt\"].values[0]),\n",
    "                \"lsm\": torch.from_numpy(asdf[\"lsm\"].values[0]),\n",
    "            }\n",
    "\n",
    "print(asdf2)\n",
    "asdf3 = asdf2\n",
    "buh = {\n",
    "                key: F.interpolate(value.unsqueeze(0).unsqueeze(0), size=(720, 1440), mode='bilinear', align_corners=False).squeeze(0).squeeze(0)\n",
    "                for key, value in asdf2.items()\n",
    "            }\n",
    "asdf.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8BjNwqwL8j3n",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from aurora import Aurora, rollout\n",
    "# from aurora.normalisation import locations, scales\n",
    "\n",
    "model = Aurora(\n",
    "    use_lora=False,\n",
    "    autocast=True,      # reduces memory usage\n",
    "    surf_vars=(\"2t\", \"10u\", \"10v\", \"msl\", \"fire\",\"lst\"),\n",
    "    static_vars=(\"lsm\", \"z\", \"slt\"),\n",
    "    atmos_vars=(\"z\", \"u\", \"v\", \"t\", \"q\"),\n",
    ")\n",
    "# model = Aurora(\n",
    "#     use_lora=False,\n",
    "#     autocast=True,      # reduces memory usage\n",
    "#     # surf_vars=(\"2t\", \"10u\", \"10v\", \"msl\"),\n",
    "#     surf_vars=(\"2t\", \"10u\", \"10v\", \"msl\", \"fire\"),\n",
    "#     static_vars=(\"lsm\", \"z\", \"slt\"),\n",
    "#     atmos_vars=(\"z\", \"u\", \"v\", \"t\", \"q\"),\n",
    "# )\n",
    "\n",
    "model.load_checkpoint(\"microsoft/aurora\", \"aurora-0.25-pretrained.ckpt\", strict=False)\n",
    "model = model.to(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# training \n",
    "model.train()\n",
    "model.configure_activation_checkpointing()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-5, weight_decay=5e-6)\n",
    "# scaler = torch.cuda.amp.GradScaler() #\n",
    "scaler = torch.amp.GradScaler('cuda') #\n",
    "\n",
    "ctr = 1\n",
    "num_epochs = 1\n",
    "\n",
    "# torch.cuda.empty_cache()\n",
    "for epoch in range(num_epochs):  # Loop over epochs\n",
    "    # model.train()  # Set model to training mode\n",
    "    for batch in dataloader:  # Iterate over batches\n",
    "        # Unpack the batch\n",
    "        features, labels = batch  # Ensure your Dataset and collate function return the right format\n",
    "        # Move data to the appropriate device\n",
    "        # features = {key: val.to(device) for key, val in features.items()}  # For dict-based features\n",
    "        labels = labels.to(device)\n",
    "        features = features.to(device)\n",
    "        # print(features.surf_vars[\"lst\"])\n",
    "        # print(features)\n",
    "        \n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            # Forward pass\n",
    "            # outputs = model(features)\n",
    "            pred = model.forward(features)\n",
    "            # print(pred.surf_vars[\"lst\"])\n",
    "            # pred is already on gpu \n",
    "            # pred = pred.to(device)[\n",
    "            # print(features)\n",
    "            # Compute the loss\n",
    "            loss = loss_func(labels,pred)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        scaler.scale(loss).backward()  # Backpropagate  nan occurs here \n",
    "        # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! gradient clipping!\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        # optimizer.step()  # Update the weights\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        # torch.cuda.empty_cache()\n",
    "        print(f\"computed batch {ctr}\")\n",
    "        ctr += 1\n",
    "        \n",
    "        # Debug memory usage\n",
    "        print(f\"Allocated: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
    "        print(f\"Reserved: {torch.cuda.memory_reserved() / 1024**2:.2f} MB\")\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "287"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## freeing memory after restarting model\n",
    "\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### some debug stuff (not important for runing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model training \n",
    "\n",
    "model.train()\n",
    "model.configure_activation_checkpointing()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-5, weight_decay=5e-6)\n",
    "\n",
    "model = model.to(device)\n",
    "model.train()\n",
    "model.configure_activation_checkpointing()\n",
    "torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "num_epochs = 1\n",
    "torch.cuda.empty_cache()\n",
    "for epoch in range(num_epochs):  # Loop over epochs\n",
    "    # model.train()  # Set model to training mode\n",
    "    for batch in dataloader:  # Iterate over batches\n",
    "        # Unpack the batch\n",
    "        features, labels = batch  # Ensure your Dataset and collate function return the right format\n",
    "        # Move data to the appropriate device\n",
    "        # features = {key: val.to(device) for key, val in features.items()}  # For dict-based features\n",
    "        labels = labels.to(device)\n",
    "        # features = features.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        # outputs = model(features)\n",
    "        pred = model.forward(features)\n",
    "        # pred is already on gpu \n",
    "        # pred = pred.to(device)[\n",
    "        print(pred)\n",
    "        # Compute the loss\n",
    "        loss = loss_func(labels,pred)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "        loss.backward()  # Backpropagate\n",
    "        optimizer.step()  # Update the weights\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"computed first batch\")\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "xprtvhnG_KTC",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch(surf_vars={'2t': tensor([[[[248.2111, 250.3261, 250.2283,  ..., 249.8091, 250.0388, 247.3506],\n",
      "          [256.5222, 255.5726, 255.3369,  ..., 255.0719, 255.2536, 245.8392],\n",
      "          [252.1862, 253.3263, 249.0282,  ..., 253.4009, 249.3633, 248.7849],\n",
      "          ...,\n",
      "          [243.7909, 243.7615, 243.8710,  ..., 243.7093, 243.9118, 244.1356],\n",
      "          [244.0923, 243.9736, 244.1088,  ..., 243.8568, 243.9820, 244.0134],\n",
      "          [243.9003, 243.9125, 243.7339,  ..., 243.9700, 243.8595, 243.8047]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>), '10u': tensor([[[[  3.5809,   3.3212,   1.3584,  ...,   3.0293,   1.0162,   3.7811],\n",
      "          [ -7.9823,  -7.5491,  -9.8704,  ...,  -7.3736,  -9.8940,  -7.1199],\n",
      "          [ -4.5390,  -4.3756, -10.2827,  ...,  -4.1192, -10.1291,  -8.2069],\n",
      "          ...,\n",
      "          [ -3.3834,  -3.3105,  -3.2968,  ...,  -3.2817,  -3.2331,  -3.3291],\n",
      "          [ -3.0977,  -3.0532,  -3.0555,  ...,  -3.1011,  -3.0014,  -3.0439],\n",
      "          [ -2.4023,  -2.4306,  -2.3970,  ...,  -2.4638,  -2.3741,  -2.2763]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>), '10v': tensor([[[[-4.8749, -1.2925, -0.4796,  ..., -1.0154, -0.1611,  0.7960],\n",
      "          [-2.6293, -1.7394, -1.4694,  ..., -1.5712, -1.2151,  3.4068],\n",
      "          [ 0.5379, -0.9354, -0.7567,  ..., -0.7018, -0.6208,  0.2830],\n",
      "          ...,\n",
      "          [-2.6891, -2.6745, -2.6719,  ..., -2.5535, -2.6514, -2.7031],\n",
      "          [-2.9315, -2.9306, -2.8654,  ..., -2.8884, -2.8488, -2.7900],\n",
      "          [-3.1042, -3.1675, -3.1209,  ..., -3.2550, -3.2691, -3.1589]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>), 'msl': tensor([[[[103155.7656, 103171.0000, 103361.3047,  ..., 103188.6484,\n",
      "           103373.1875, 103779.3672],\n",
      "          [103120.0859, 103376.4766, 103393.4688,  ..., 103378.6484,\n",
      "           103400.9844, 103553.7812],\n",
      "          [103412.0781, 103079.2734, 103266.9062,  ..., 103085.9062,\n",
      "           103279.3438, 103561.0078],\n",
      "          ...,\n",
      "          [100833.1016, 100828.8125, 100829.6875,  ..., 100808.6406,\n",
      "           100802.8594, 100802.4453],\n",
      "          [100774.3750, 100774.4766, 100769.6016,  ..., 100749.1953,\n",
      "           100748.4609, 100745.7109],\n",
      "          [100688.2344, 100688.4688, 100685.4922,  ..., 100670.0000,\n",
      "           100671.0859, 100670.5078]]]], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>), 'fire': tensor([[[[ 1.5725,  0.3635,  0.8169,  ..., -0.1089,  0.8450, -0.9450],\n",
      "          [-3.1510,  1.6862,  0.6224,  ...,  1.7762,  1.0274,  1.3167],\n",
      "          [-0.6240, -1.8756, -0.5867,  ..., -1.8165, -0.6199,  1.8486],\n",
      "          ...,\n",
      "          [-0.2371, -1.0532,  0.0192,  ..., -1.2139,  0.2721,  0.7380],\n",
      "          [-0.1559, -0.3128, -0.3330,  ...,  0.2921, -0.4360, -0.2993],\n",
      "          [ 0.9598, -0.6867,  0.8512,  ..., -0.9127,  0.7003, -0.1197]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>), 'lst': tensor([[[[ 4.8698e+03,  1.5610e+04,  1.5298e+04,  ...,  1.5687e+04,\n",
      "            1.1844e+04, -5.5840e+00],\n",
      "          [ 3.0194e+04,  8.0828e+03,  3.5916e+04,  ...,  1.0033e+04,\n",
      "            3.8131e+04,  1.9069e+04],\n",
      "          [-6.9910e+03,  3.3224e+04,  1.3953e+04,  ...,  3.1893e+04,\n",
      "            1.5527e+04,  3.5842e+03],\n",
      "          ...,\n",
      "          [ 1.4429e+04,  1.1851e+04,  1.4179e+04,  ...,  1.3554e+04,\n",
      "            1.5518e+04,  1.2642e+04],\n",
      "          [ 9.4710e+03,  1.5647e+04,  1.1613e+04,  ...,  1.5155e+04,\n",
      "            1.1074e+04,  6.2063e+02],\n",
      "          [ 1.9149e+04,  7.6491e+03,  8.3493e+03,  ...,  6.5968e+03,\n",
      "            8.6703e+03,  7.4523e+03]]]], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)}, static_vars={'z': tensor([[ 3.2275e-01,  3.2275e-01,  3.2275e-01,  ...,  3.2275e-01,\n",
      "          3.2275e-01,  3.2275e-01],\n",
      "        [ 4.6157e+00,  4.6040e+00,  4.5884e+00,  ...,  4.7915e+00,\n",
      "          4.7329e+00,  4.6743e+00],\n",
      "        [-1.7710e+00, -1.7163e+00, -1.7085e+00,  ..., -1.5093e+00,\n",
      "         -1.6226e+00, -1.7124e+00],\n",
      "        ...,\n",
      "        [ 2.6638e+04,  2.6642e+04,  2.6647e+04,  ...,  2.6627e+04,\n",
      "          2.6631e+04,  2.6634e+04],\n",
      "        [ 2.6958e+04,  2.6961e+04,  2.6965e+04,  ...,  2.6950e+04,\n",
      "          2.6953e+04,  2.6956e+04],\n",
      "        [ 2.7163e+04,  2.7165e+04,  2.7167e+04,  ...,  2.7159e+04,\n",
      "          2.7160e+04,  2.7162e+04]], device='cuda:0'), 'slt': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0'), 'lsm': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0')}, atmos_vars={'t': tensor([[[[[253.0678, 252.9532, 252.9864,  ..., 251.7806, 251.8700,\n",
      "            251.9760],\n",
      "           [253.4178, 253.4042, 253.4882,  ..., 252.3365, 252.4279,\n",
      "            252.4541],\n",
      "           [253.7226, 253.7266, 253.7775,  ..., 252.9057, 252.9408,\n",
      "            252.9969],\n",
      "           ...,\n",
      "           [261.3773, 261.2387, 261.1628,  ..., 261.0561, 260.9697,\n",
      "            260.8234],\n",
      "           [261.9394, 261.8354, 261.8147,  ..., 261.9169, 261.9153,\n",
      "            261.9816],\n",
      "           [262.4042, 262.3528, 262.3931,  ..., 262.5049, 262.5680,\n",
      "            262.7904]],\n",
      "\n",
      "          [[252.4198, 252.3776, 252.4197,  ..., 252.1685, 252.2280,\n",
      "            252.2712],\n",
      "           [253.0806, 253.0798, 253.1377,  ..., 252.8577, 252.9245,\n",
      "            252.8753],\n",
      "           [253.7554, 253.7547, 253.7312,  ..., 253.5182, 253.4987,\n",
      "            253.5080],\n",
      "           ...,\n",
      "           [258.3848, 258.4144, 258.4585,  ..., 258.3852, 258.4243,\n",
      "            258.3203],\n",
      "           [258.8112, 258.8616, 258.9091,  ..., 258.9735, 259.0275,\n",
      "            259.0441],\n",
      "           [259.4121, 259.4465, 259.4864,  ..., 259.5492, 259.6006,\n",
      "            259.7372]],\n",
      "\n",
      "          [[252.9567, 252.9086, 252.9028,  ..., 252.6964, 252.7074,\n",
      "            252.6855],\n",
      "           [252.6675, 252.6472, 252.6939,  ..., 252.5457, 252.6081,\n",
      "            252.5219],\n",
      "           [252.5254, 252.5084, 252.5084,  ..., 252.4644, 252.4772,\n",
      "            252.4867],\n",
      "           ...,\n",
      "           [262.9765, 262.9539, 263.0072,  ..., 263.0193, 263.0688,\n",
      "            263.0918],\n",
      "           [263.2308, 263.1952, 263.2193,  ..., 263.2718, 263.3070,\n",
      "            263.3098],\n",
      "           [263.7971, 263.7460, 263.6963,  ..., 263.8141, 263.7764,\n",
      "            263.8120]],\n",
      "\n",
      "          ...,\n",
      "\n",
      "          [[215.7774, 215.7905, 215.7873,  ..., 215.8403, 215.8383,\n",
      "            215.8670],\n",
      "           [215.6535, 215.6604, 215.6744,  ..., 215.7962, 215.8080,\n",
      "            215.8305],\n",
      "           [215.5596, 215.5681, 215.5955,  ..., 215.7561, 215.7763,\n",
      "            215.7827],\n",
      "           ...,\n",
      "           [222.9413, 222.8996, 222.8862,  ..., 222.9042, 222.8959,\n",
      "            222.8888],\n",
      "           [223.0718, 223.0785, 223.1065,  ..., 223.0714, 223.1014,\n",
      "            223.1091],\n",
      "           [223.4280, 223.4832, 223.5437,  ..., 223.4890, 223.5499,\n",
      "            223.5925]],\n",
      "\n",
      "          [[218.2388, 218.2704, 218.3143,  ..., 218.0867, 218.1241,\n",
      "            218.1765],\n",
      "           [218.1566, 218.1404, 218.1792,  ..., 217.8495, 217.8852,\n",
      "            217.9456],\n",
      "           [218.0021, 217.9844, 218.0129,  ..., 217.6137, 217.6475,\n",
      "            217.6447],\n",
      "           ...,\n",
      "           [219.7470, 219.7549, 219.7881,  ..., 219.4391, 219.4781,\n",
      "            219.5555],\n",
      "           [219.6489, 219.5802, 219.5976,  ..., 219.3261, 219.3538,\n",
      "            219.4439],\n",
      "           [219.5512, 219.4889, 219.4921,  ..., 219.3181, 219.3329,\n",
      "            219.4777]],\n",
      "\n",
      "          [[219.1212, 219.0997, 219.1038,  ..., 219.1952, 219.1909,\n",
      "            219.2191],\n",
      "           [219.0377, 218.9825, 218.9808,  ..., 219.1083, 219.1022,\n",
      "            219.1252],\n",
      "           [218.9314, 218.8918, 218.8762,  ..., 219.0464, 219.0352,\n",
      "            218.9960],\n",
      "           ...,\n",
      "           [220.3796, 220.3489, 220.3890,  ..., 219.9069, 219.9408,\n",
      "            219.9239],\n",
      "           [220.2174, 220.2134, 220.2170,  ..., 219.7851, 219.7827,\n",
      "            219.7500],\n",
      "           [220.1246, 220.1093, 220.0896,  ..., 219.7009, 219.6754,\n",
      "            219.6900]]]]], device='cuda:0', grad_fn=<AddBackward0>), 'u': tensor([[[[[ 1.0762e+00,  1.0854e+00,  1.1057e+00,  ...,  1.0287e+00,\n",
      "             1.0687e+00,  1.0801e+00],\n",
      "           [ 6.5479e-01,  6.3843e-01,  6.6259e-01,  ...,  4.5452e-01,\n",
      "             4.8658e-01,  5.4663e-01],\n",
      "           [-1.5553e+00, -1.5866e+00, -1.5640e+00,  ..., -1.5490e+00,\n",
      "            -1.5160e+00, -1.4330e+00],\n",
      "           ...,\n",
      "           [-4.2356e+00, -4.1951e+00, -4.0891e+00,  ..., -4.0725e+00,\n",
      "            -3.9621e+00, -3.8797e+00],\n",
      "           [-4.1658e+00, -4.1273e+00, -4.0059e+00,  ..., -4.1724e+00,\n",
      "            -4.0091e+00, -3.8773e+00],\n",
      "           [-3.6949e+00, -3.6706e+00, -3.5840e+00,  ..., -3.7512e+00,\n",
      "            -3.6220e+00, -3.5105e+00]],\n",
      "\n",
      "          [[ 7.2803e-01,  7.2235e-01,  7.7324e-01,  ...,  5.8227e-01,\n",
      "             6.2560e-01,  6.5693e-01],\n",
      "           [ 2.7366e+00,  2.7021e+00,  2.7070e+00,  ...,  2.4770e+00,\n",
      "             2.4691e+00,  2.4731e+00],\n",
      "           [ 9.2238e-01,  8.9103e-01,  8.7025e-01,  ...,  8.7683e-01,\n",
      "             8.4745e-01,  8.2801e-01],\n",
      "           ...,\n",
      "           [-4.4971e+00, -4.5013e+00, -4.5235e+00,  ..., -4.4392e+00,\n",
      "            -4.4652e+00, -4.4755e+00],\n",
      "           [-4.0764e+00, -4.1040e+00, -4.1268e+00,  ..., -4.0923e+00,\n",
      "            -4.1087e+00, -4.1354e+00],\n",
      "           [-3.5986e+00, -3.6421e+00, -3.6747e+00,  ..., -3.6700e+00,\n",
      "            -3.6863e+00, -3.6594e+00]],\n",
      "\n",
      "          [[ 9.3481e-01,  9.1480e-01,  9.3193e-01,  ...,  7.8862e-01,\n",
      "             8.1288e-01,  8.4617e-01],\n",
      "           [ 3.5463e+00,  3.5405e+00,  3.5248e+00,  ...,  3.3973e+00,\n",
      "             3.3899e+00,  3.3794e+00],\n",
      "           [ 2.0491e+00,  2.0626e+00,  2.0417e+00,  ...,  2.0445e+00,\n",
      "             2.0334e+00,  1.9801e+00],\n",
      "           ...,\n",
      "           [-4.0314e+00, -4.1169e+00, -4.1879e+00,  ..., -3.9975e+00,\n",
      "            -4.0748e+00, -4.0924e+00],\n",
      "           [-3.8320e+00, -3.8847e+00, -3.8979e+00,  ..., -3.8697e+00,\n",
      "            -3.8847e+00, -3.9059e+00],\n",
      "           [-3.4170e+00, -3.4484e+00, -3.4453e+00,  ..., -3.4643e+00,\n",
      "            -3.4507e+00, -3.4422e+00]],\n",
      "\n",
      "          ...,\n",
      "\n",
      "          [[-3.8785e-01, -4.8306e-01, -5.2313e-01,  ..., -7.6543e-01,\n",
      "            -8.2480e-01, -9.0477e-01],\n",
      "           [-1.2112e+01, -1.2100e+01, -1.2170e+01,  ..., -1.1839e+01,\n",
      "            -1.1953e+01, -1.2084e+01],\n",
      "           [-1.2122e+01, -1.2142e+01, -1.2079e+01,  ..., -1.1907e+01,\n",
      "            -1.1873e+01, -1.1962e+01],\n",
      "           ...,\n",
      "           [-1.8366e+00, -1.9865e+00, -2.1571e+00,  ..., -2.0648e+00,\n",
      "            -2.2363e+00, -2.4399e+00],\n",
      "           [-1.5947e+00, -1.6515e+00, -1.6455e+00,  ..., -1.6683e+00,\n",
      "            -1.6643e+00, -1.7934e+00],\n",
      "           [-1.4896e+00, -1.5200e+00, -1.4860e+00,  ..., -1.4557e+00,\n",
      "            -1.4316e+00, -1.4954e+00]],\n",
      "\n",
      "          [[-3.7271e-01, -3.8045e-01, -3.7532e-01,  ...,  5.9234e-02,\n",
      "             2.8181e-02,  6.7933e-02],\n",
      "           [-2.4821e+01, -2.4825e+01, -2.4802e+01,  ..., -2.4214e+01,\n",
      "            -2.4219e+01, -2.4066e+01],\n",
      "           [-2.4013e+01, -2.4160e+01, -2.4109e+01,  ..., -2.3522e+01,\n",
      "            -2.3467e+01, -2.3267e+01],\n",
      "           ...,\n",
      "           [-2.3643e+00, -2.7638e+00, -3.1109e+00,  ..., -2.4837e+00,\n",
      "            -2.8030e+00, -3.0901e+00],\n",
      "           [-2.9680e+00, -3.2458e+00, -3.1576e+00,  ..., -3.0336e+00,\n",
      "            -2.9139e+00, -2.8525e+00],\n",
      "           [-3.5974e+00, -3.6203e+00, -3.2345e+00,  ..., -3.3374e+00,\n",
      "            -2.9478e+00, -2.4521e+00]],\n",
      "\n",
      "          [[-1.2704e+00, -1.2701e+00, -1.2004e+00,  ..., -4.0249e-01,\n",
      "            -3.8435e-01, -3.4056e-01],\n",
      "           [-2.9602e+01, -2.9558e+01, -2.9516e+01,  ..., -2.9463e+01,\n",
      "            -2.9479e+01, -2.9442e+01],\n",
      "           [-2.9224e+01, -2.9253e+01, -2.9133e+01,  ..., -2.9136e+01,\n",
      "            -2.9038e+01, -2.8901e+01],\n",
      "           ...,\n",
      "           [-3.5017e+00, -3.9833e+00, -4.5368e+00,  ..., -3.8546e+00,\n",
      "            -4.4206e+00, -4.8193e+00],\n",
      "           [-4.5844e+00, -4.8548e+00, -4.8941e+00,  ..., -4.7224e+00,\n",
      "            -4.7655e+00, -4.6885e+00],\n",
      "           [-5.3787e+00, -5.3912e+00, -5.1718e+00,  ..., -5.3021e+00,\n",
      "            -5.0679e+00, -4.6876e+00]]]]], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>), 'v': tensor([[[[[-0.1439, -0.0378,  0.0457,  ...,  0.0266,  0.1196,  0.2052],\n",
      "           [ 0.7476,  0.8870,  0.9589,  ...,  0.6310,  0.6939,  0.7194],\n",
      "           [ 0.8458,  0.9882,  1.0462,  ...,  0.6539,  0.7319,  0.7490],\n",
      "           ...,\n",
      "           [-1.6353, -1.8233, -2.0816,  ..., -1.8241, -2.1004, -2.3597],\n",
      "           [-2.0015, -2.1386, -2.3180,  ..., -2.0367, -2.2335, -2.4103],\n",
      "           [-1.9367, -2.0118, -2.0975,  ..., -1.9860, -2.0938, -2.2244]],\n",
      "\n",
      "          [[ 0.0601,  0.1564,  0.1570,  ...,  0.0856,  0.1179,  0.0891],\n",
      "           [ 0.1849,  0.2665,  0.2774,  ...,  0.3833,  0.4069,  0.3832],\n",
      "           [ 0.3660,  0.4366,  0.4830,  ...,  0.4886,  0.5375,  0.5621],\n",
      "           ...,\n",
      "           [-1.8977, -2.0183, -2.1157,  ..., -1.9929, -2.0834, -2.2128],\n",
      "           [-2.2929, -2.3458, -2.3735,  ..., -2.2868, -2.3165, -2.4034],\n",
      "           [-2.6694, -2.6601, -2.6430,  ..., -2.6631, -2.6664, -2.7598]],\n",
      "\n",
      "          [[ 0.3854,  0.4470,  0.4661,  ...,  0.3571,  0.4140,  0.4417],\n",
      "           [-0.2293, -0.1463, -0.1009,  ..., -0.1686, -0.1099, -0.0748],\n",
      "           [-0.2768, -0.1530, -0.0562,  ..., -0.1243, -0.0369,  0.0274],\n",
      "           ...,\n",
      "           [-2.1320, -2.1311, -2.1602,  ..., -2.0430, -2.0471, -2.0791],\n",
      "           [-2.3685, -2.3561, -2.3843,  ..., -2.2663, -2.2791, -2.3407],\n",
      "           [-2.5970, -2.5903, -2.6080,  ..., -2.6368, -2.6449, -2.6743]],\n",
      "\n",
      "          ...,\n",
      "\n",
      "          [[ 0.4233,  0.3810,  0.4165,  ...,  0.4108,  0.4842,  0.6207],\n",
      "           [-3.2482, -3.2240, -3.1537,  ..., -4.1391, -4.0416, -3.9070],\n",
      "           [-2.4103, -2.4057, -2.3373,  ..., -3.2102, -3.1302, -3.0032],\n",
      "           ...,\n",
      "           [ 1.4330,  1.4266,  1.4371,  ...,  1.5219,  1.5354,  1.5183],\n",
      "           [ 1.4096,  1.3056,  1.1992,  ...,  1.3715,  1.2651,  1.1739],\n",
      "           [ 1.6519,  1.4228,  1.2342,  ...,  1.4253,  1.2365,  0.9679]],\n",
      "\n",
      "          [[-0.1239, -0.2172, -0.2713,  ..., -0.2169, -0.2517, -0.2349],\n",
      "           [-4.8543, -4.8110, -4.7477,  ..., -5.5473, -5.4755, -5.3468],\n",
      "           [-4.5133, -4.4932, -4.4277,  ..., -5.2759, -5.2028, -5.0800],\n",
      "           ...,\n",
      "           [ 0.2944,  0.2050,  0.0800,  ...,  0.1119,  0.0190, -0.1348],\n",
      "           [ 0.0878, -0.1579, -0.4043,  ..., -0.2968, -0.5259, -0.8251],\n",
      "           [ 0.0233, -0.2702, -0.6823,  ..., -0.5118, -0.9119, -1.3365]],\n",
      "\n",
      "          [[-0.2060, -0.2902, -0.3001,  ..., -0.3867, -0.4228, -0.4177],\n",
      "           [-2.3641, -2.3090, -2.1760,  ..., -2.7494, -2.6425, -2.5142],\n",
      "           [-2.2168, -2.1785, -2.0516,  ..., -2.5424, -2.4287, -2.3294],\n",
      "           ...,\n",
      "           [ 0.5596,  0.2652, -0.0197,  ...,  0.4764,  0.1967, -0.1468],\n",
      "           [ 0.5612,  0.2917,  0.0341,  ...,  0.4758,  0.2232, -0.0168],\n",
      "           [ 0.6654,  0.4110,  0.1279,  ...,  0.5464,  0.2811,  0.0583]]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>), 'q': tensor([[[[[4.9426e-04, 5.1562e-04, 5.2646e-04,  ..., 4.5729e-04,\n",
      "            4.7769e-04, 4.7802e-04],\n",
      "           [5.3328e-04, 5.4159e-04, 5.4558e-04,  ..., 4.8678e-04,\n",
      "            4.9476e-04, 5.0067e-04],\n",
      "           [5.4074e-04, 5.5389e-04, 5.3410e-04,  ..., 5.0518e-04,\n",
      "            4.8393e-04, 4.8613e-04],\n",
      "           ...,\n",
      "           [5.2098e-04, 5.4168e-04, 5.6810e-04,  ..., 5.5508e-04,\n",
      "            5.8315e-04, 5.8383e-04],\n",
      "           [4.6724e-04, 5.0790e-04, 5.2666e-04,  ..., 5.1258e-04,\n",
      "            5.3761e-04, 5.5207e-04],\n",
      "           [4.6079e-04, 4.8549e-04, 5.1996e-04,  ..., 4.7414e-04,\n",
      "            5.1691e-04, 5.3847e-04]],\n",
      "\n",
      "          [[4.8316e-04, 5.0846e-04, 5.2829e-04,  ..., 5.1613e-04,\n",
      "            5.3825e-04, 5.3470e-04],\n",
      "           [5.3821e-04, 5.4529e-04, 5.6039e-04,  ..., 5.6506e-04,\n",
      "            5.7669e-04, 5.8269e-04],\n",
      "           [5.7304e-04, 5.8800e-04, 5.7793e-04,  ..., 6.1407e-04,\n",
      "            5.9796e-04, 5.9863e-04],\n",
      "           ...,\n",
      "           [4.2458e-04, 4.1608e-04, 4.2025e-04,  ..., 4.1749e-04,\n",
      "            4.2603e-04, 4.3917e-04],\n",
      "           [3.6894e-04, 3.9303e-04, 3.8764e-04,  ..., 3.9268e-04,\n",
      "            3.9052e-04, 4.0247e-04],\n",
      "           [3.4996e-04, 3.5526e-04, 3.7220e-04,  ..., 3.6206e-04,\n",
      "            3.7687e-04, 3.8701e-04]],\n",
      "\n",
      "          [[1.9930e-04, 2.0702e-04, 2.1056e-04,  ..., 2.1528e-04,\n",
      "            2.1253e-04, 1.9432e-04],\n",
      "           [2.3547e-04, 2.3308e-04, 2.3489e-04,  ..., 2.5215e-04,\n",
      "            2.4632e-04, 2.3693e-04],\n",
      "           [2.8267e-04, 2.9178e-04, 2.7767e-04,  ..., 3.1301e-04,\n",
      "            2.9244e-04, 2.7613e-04],\n",
      "           ...,\n",
      "           [6.8323e-04, 6.7809e-04, 6.5805e-04,  ..., 6.7973e-04,\n",
      "            6.5955e-04, 6.3310e-04],\n",
      "           [6.8578e-04, 7.0348e-04, 6.8047e-04,  ..., 7.1211e-04,\n",
      "            6.8846e-04, 6.7504e-04],\n",
      "           [6.8027e-04, 7.0837e-04, 7.2111e-04,  ..., 7.3787e-04,\n",
      "            7.5745e-04, 7.6570e-04]],\n",
      "\n",
      "          ...,\n",
      "\n",
      "          [[4.0416e-06, 4.2117e-06, 4.3480e-06,  ..., 4.2685e-06,\n",
      "            4.3593e-06, 4.2851e-06],\n",
      "           [4.0513e-06, 4.0884e-06, 4.1855e-06,  ..., 4.0426e-06,\n",
      "            4.1171e-06, 4.0518e-06],\n",
      "           [3.9099e-06, 3.9317e-06, 3.9435e-06,  ..., 3.8096e-06,\n",
      "            3.8109e-06, 3.8372e-06],\n",
      "           ...,\n",
      "           [2.5676e-06, 2.5066e-06, 2.5761e-06,  ..., 2.5761e-06,\n",
      "            2.6661e-06, 2.6758e-06],\n",
      "           [2.3468e-06, 2.2202e-06, 2.2297e-06,  ..., 2.2595e-06,\n",
      "            2.2781e-06, 2.3795e-06],\n",
      "           [2.4962e-06, 2.2913e-06, 2.1947e-06,  ..., 2.3323e-06,\n",
      "            2.2140e-06, 2.2276e-06]],\n",
      "\n",
      "          [[2.8873e-06, 2.8855e-06, 2.8813e-06,  ..., 2.8858e-06,\n",
      "            2.8817e-06, 2.8761e-06],\n",
      "           [2.8658e-06, 2.8638e-06, 2.8620e-06,  ..., 2.8627e-06,\n",
      "            2.8608e-06, 2.8562e-06],\n",
      "           [2.8390e-06, 2.8372e-06, 2.8375e-06,  ..., 2.8356e-06,\n",
      "            2.8350e-06, 2.8313e-06],\n",
      "           ...,\n",
      "           [2.4948e-06, 2.5120e-06, 2.5156e-06,  ..., 2.5084e-06,\n",
      "            2.5144e-06, 2.5042e-06],\n",
      "           [2.4615e-06, 2.4780e-06, 2.4889e-06,  ..., 2.4872e-06,\n",
      "            2.4979e-06, 2.4963e-06],\n",
      "           [2.4511e-06, 2.4604e-06, 2.4703e-06,  ..., 2.4719e-06,\n",
      "            2.4800e-06, 2.4747e-06]],\n",
      "\n",
      "          [[2.8886e-06, 2.8890e-06, 2.8895e-06,  ..., 2.8843e-06,\n",
      "            2.8847e-06, 2.8852e-06],\n",
      "           [2.8900e-06, 2.8899e-06, 2.8904e-06,  ..., 2.8885e-06,\n",
      "            2.8889e-06, 2.8882e-06],\n",
      "           [2.8864e-06, 2.8857e-06, 2.8863e-06,  ..., 2.8879e-06,\n",
      "            2.8887e-06, 2.8889e-06],\n",
      "           ...,\n",
      "           [2.5637e-06, 2.5780e-06, 2.5852e-06,  ..., 2.5829e-06,\n",
      "            2.5894e-06, 2.5849e-06],\n",
      "           [2.5443e-06, 2.5597e-06, 2.5709e-06,  ..., 2.5653e-06,\n",
      "            2.5759e-06, 2.5760e-06],\n",
      "           [2.5434e-06, 2.5494e-06, 2.5517e-06,  ..., 2.5540e-06,\n",
      "            2.5569e-06, 2.5532e-06]]]]], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>), 'z': tensor([[[[[  1761.7148,   1764.4604,   1766.3538,  ...,   1787.5542,\n",
      "              1787.4548,   1783.9109],\n",
      "           [  1786.0374,   1788.0220,   1788.5989,  ...,   1798.6658,\n",
      "              1798.3684,   1797.7231],\n",
      "           [  1797.9878,   1798.1343,   1802.0554,  ...,   1800.7249,\n",
      "              1804.8792,   1803.4526],\n",
      "           ...,\n",
      "           [   344.1989,    345.8950,    347.1611,  ...,    354.2242,\n",
      "               356.3124,    354.1271],\n",
      "           [   315.5240,    315.6721,    311.7759,  ...,    312.7455,\n",
      "               309.0732,    305.5089],\n",
      "           [   292.1935,    287.4634,    279.4576,  ...,    287.6305,\n",
      "               279.9420,    273.5722]],\n",
      "\n",
      "          [[  7428.7642,   7429.7104,   7431.8838,  ...,   7433.7246,\n",
      "              7435.1704,   7435.8359],\n",
      "           [  7456.6479,   7456.3633,   7456.9067,  ...,   7456.0195,\n",
      "              7456.1450,   7455.7988],\n",
      "           [  7475.6357,   7475.4448,   7476.9043,  ...,   7471.5566,\n",
      "              7472.8281,   7472.9858],\n",
      "           ...,\n",
      "           [  6279.3071,   6277.7061,   6279.8838,  ...,   6275.6484,\n",
      "              6278.7324,   6280.0908],\n",
      "           [  6257.7695,   6258.8730,   6260.1255,  ...,   6252.8701,\n",
      "              6254.8555,   6256.1851],\n",
      "           [  6231.8765,   6234.0840,   6236.5898,  ...,   6236.3247,\n",
      "              6239.0308,   6237.9790]],\n",
      "\n",
      "          [[ 13582.7354,  13585.0078,  13585.2041,  ...,  13583.2393,\n",
      "             13582.8945,  13582.6191],\n",
      "           [ 13608.7061,  13610.2031,  13610.6484,  ...,  13605.8838,\n",
      "             13605.7744,  13606.6670],\n",
      "           [ 13629.3174,  13628.8506,  13630.9160,  ...,  13622.3389,\n",
      "             13623.9785,  13623.2207],\n",
      "           ...,\n",
      "           [ 12695.2959,  12693.5430,  12698.5918,  ...,  12687.1064,\n",
      "             12692.9502,  12696.6182],\n",
      "           [ 12686.7051,  12689.2871,  12690.2744,  ...,  12680.5029,\n",
      "             12681.9297,  12687.2910],\n",
      "           [ 12668.9355,  12673.3105,  12678.2764,  ...,  12667.9004,\n",
      "             12673.1045,  12673.7959]],\n",
      "\n",
      "          ...,\n",
      "\n",
      "          [[108562.4922, 108575.3359, 108566.7812,  ..., 108572.2031,\n",
      "            108563.4141, 108568.6641],\n",
      "           [108494.4844, 108494.6250, 108501.6484,  ..., 108495.4688,\n",
      "            108502.7344, 108536.7578],\n",
      "           [108475.5781, 108464.7578, 108483.3203,  ..., 108470.1875,\n",
      "            108489.5312, 108473.3047],\n",
      "           ...,\n",
      "           [108991.2812, 108983.8672, 108995.6328,  ..., 109022.0547,\n",
      "            109034.7969, 109046.9219],\n",
      "           [108996.4297, 109007.1016, 109009.7500,  ..., 109045.0547,\n",
      "            109049.1797, 109058.5625],\n",
      "           [108994.1641, 109004.1953, 109014.2188,  ..., 109043.0781,\n",
      "            109054.8906, 109055.7891]],\n",
      "\n",
      "          [[151849.5000, 151858.5469, 151855.0156,  ..., 151867.1094,\n",
      "            151863.9375, 151876.0312],\n",
      "           [151726.7812, 151728.8438, 151733.6094,  ..., 151744.2969,\n",
      "            151749.0625, 151776.2188],\n",
      "           [151649.7656, 151650.6562, 151661.2031,  ..., 151671.2344,\n",
      "            151680.8438, 151670.4375],\n",
      "           ...,\n",
      "           [153666.8906, 153660.9531, 153667.0625,  ..., 153697.9531,\n",
      "            153706.0781, 153689.9219],\n",
      "           [153645.9688, 153667.7344, 153667.6094,  ..., 153701.2656,\n",
      "            153703.3281, 153712.5938],\n",
      "           [153619.0000, 153636.4375, 153646.7656,  ..., 153666.2188,\n",
      "            153679.5469, 153678.7188]],\n",
      "\n",
      "          [[195122.4531, 195116.5625, 195119.7969,  ..., 195105.7188,\n",
      "            195108.6875, 195126.4062],\n",
      "           [194978.0469, 194973.2031, 194978.6562,  ..., 194972.2500,\n",
      "            194976.4531, 194984.1250],\n",
      "           [194844.2188, 194861.0469, 194855.2969,  ..., 194869.4062,\n",
      "            194862.3906, 194870.5625],\n",
      "           ...,\n",
      "           [196654.6250, 196658.1094, 196671.3438,  ..., 196638.2500,\n",
      "            196655.7812, 196652.0625],\n",
      "           [196654.4688, 196686.3125, 196692.3906,  ..., 196658.9688,\n",
      "            196667.5312, 196704.0625],\n",
      "           [196666.7344, 196681.0469, 196707.0156,  ..., 196646.6094,\n",
      "            196674.8906, 196689.9219]]]]], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)}, metadata=Metadata(lat=tensor([ 90.0000,  89.7500,  89.5000,  89.2500,  89.0000,  88.7500,  88.5000,\n",
      "         88.2500,  88.0000,  87.7500,  87.5000,  87.2500,  87.0000,  86.7500,\n",
      "         86.5000,  86.2500,  86.0000,  85.7500,  85.5000,  85.2500,  85.0000,\n",
      "         84.7500,  84.5000,  84.2500,  84.0000,  83.7500,  83.5000,  83.2500,\n",
      "         83.0000,  82.7500,  82.5000,  82.2500,  82.0000,  81.7500,  81.5000,\n",
      "         81.2500,  81.0000,  80.7500,  80.5000,  80.2500,  80.0000,  79.7500,\n",
      "         79.5000,  79.2500,  79.0000,  78.7500,  78.5000,  78.2500,  78.0000,\n",
      "         77.7500,  77.5000,  77.2500,  77.0000,  76.7500,  76.5000,  76.2500,\n",
      "         76.0000,  75.7500,  75.5000,  75.2500,  75.0000,  74.7500,  74.5000,\n",
      "         74.2500,  74.0000,  73.7500,  73.5000,  73.2500,  73.0000,  72.7500,\n",
      "         72.5000,  72.2500,  72.0000,  71.7500,  71.5000,  71.2500,  71.0000,\n",
      "         70.7500,  70.5000,  70.2500,  70.0000,  69.7500,  69.5000,  69.2500,\n",
      "         69.0000,  68.7500,  68.5000,  68.2500,  68.0000,  67.7500,  67.5000,\n",
      "         67.2500,  67.0000,  66.7500,  66.5000,  66.2500,  66.0000,  65.7500,\n",
      "         65.5000,  65.2500,  65.0000,  64.7500,  64.5000,  64.2500,  64.0000,\n",
      "         63.7500,  63.5000,  63.2500,  63.0000,  62.7500,  62.5000,  62.2500,\n",
      "         62.0000,  61.7500,  61.5000,  61.2500,  61.0000,  60.7500,  60.5000,\n",
      "         60.2500,  60.0000,  59.7500,  59.5000,  59.2500,  59.0000,  58.7500,\n",
      "         58.5000,  58.2500,  58.0000,  57.7500,  57.5000,  57.2500,  57.0000,\n",
      "         56.7500,  56.5000,  56.2500,  56.0000,  55.7500,  55.5000,  55.2500,\n",
      "         55.0000,  54.7500,  54.5000,  54.2500,  54.0000,  53.7500,  53.5000,\n",
      "         53.2500,  53.0000,  52.7500,  52.5000,  52.2500,  52.0000,  51.7500,\n",
      "         51.5000,  51.2500,  51.0000,  50.7500,  50.5000,  50.2500,  50.0000,\n",
      "         49.7500,  49.5000,  49.2500,  49.0000,  48.7500,  48.5000,  48.2500,\n",
      "         48.0000,  47.7500,  47.5000,  47.2500,  47.0000,  46.7500,  46.5000,\n",
      "         46.2500,  46.0000,  45.7500,  45.5000,  45.2500,  45.0000,  44.7500,\n",
      "         44.5000,  44.2500,  44.0000,  43.7500,  43.5000,  43.2500,  43.0000,\n",
      "         42.7500,  42.5000,  42.2500,  42.0000,  41.7500,  41.5000,  41.2500,\n",
      "         41.0000,  40.7500,  40.5000,  40.2500,  40.0000,  39.7500,  39.5000,\n",
      "         39.2500,  39.0000,  38.7500,  38.5000,  38.2500,  38.0000,  37.7500,\n",
      "         37.5000,  37.2500,  37.0000,  36.7500,  36.5000,  36.2500,  36.0000,\n",
      "         35.7500,  35.5000,  35.2500,  35.0000,  34.7500,  34.5000,  34.2500,\n",
      "         34.0000,  33.7500,  33.5000,  33.2500,  33.0000,  32.7500,  32.5000,\n",
      "         32.2500,  32.0000,  31.7500,  31.5000,  31.2500,  31.0000,  30.7500,\n",
      "         30.5000,  30.2500,  30.0000,  29.7500,  29.5000,  29.2500,  29.0000,\n",
      "         28.7500,  28.5000,  28.2500,  28.0000,  27.7500,  27.5000,  27.2500,\n",
      "         27.0000,  26.7500,  26.5000,  26.2500,  26.0000,  25.7500,  25.5000,\n",
      "         25.2500,  25.0000,  24.7500,  24.5000,  24.2500,  24.0000,  23.7500,\n",
      "         23.5000,  23.2500,  23.0000,  22.7500,  22.5000,  22.2500,  22.0000,\n",
      "         21.7500,  21.5000,  21.2500,  21.0000,  20.7500,  20.5000,  20.2500,\n",
      "         20.0000,  19.7500,  19.5000,  19.2500,  19.0000,  18.7500,  18.5000,\n",
      "         18.2500,  18.0000,  17.7500,  17.5000,  17.2500,  17.0000,  16.7500,\n",
      "         16.5000,  16.2500,  16.0000,  15.7500,  15.5000,  15.2500,  15.0000,\n",
      "         14.7500,  14.5000,  14.2500,  14.0000,  13.7500,  13.5000,  13.2500,\n",
      "         13.0000,  12.7500,  12.5000,  12.2500,  12.0000,  11.7500,  11.5000,\n",
      "         11.2500,  11.0000,  10.7500,  10.5000,  10.2500,  10.0000,   9.7500,\n",
      "          9.5000,   9.2500,   9.0000,   8.7500,   8.5000,   8.2500,   8.0000,\n",
      "          7.7500,   7.5000,   7.2500,   7.0000,   6.7500,   6.5000,   6.2500,\n",
      "          6.0000,   5.7500,   5.5000,   5.2500,   5.0000,   4.7500,   4.5000,\n",
      "          4.2500,   4.0000,   3.7500,   3.5000,   3.2500,   3.0000,   2.7500,\n",
      "          2.5000,   2.2500,   2.0000,   1.7500,   1.5000,   1.2500,   1.0000,\n",
      "          0.7500,   0.5000,   0.2500,   0.0000,  -0.2500,  -0.5000,  -0.7500,\n",
      "         -1.0000,  -1.2500,  -1.5000,  -1.7500,  -2.0000,  -2.2500,  -2.5000,\n",
      "         -2.7500,  -3.0000,  -3.2500,  -3.5000,  -3.7500,  -4.0000,  -4.2500,\n",
      "         -4.5000,  -4.7500,  -5.0000,  -5.2500,  -5.5000,  -5.7500,  -6.0000,\n",
      "         -6.2500,  -6.5000,  -6.7500,  -7.0000,  -7.2500,  -7.5000,  -7.7500,\n",
      "         -8.0000,  -8.2500,  -8.5000,  -8.7500,  -9.0000,  -9.2500,  -9.5000,\n",
      "         -9.7500, -10.0000, -10.2500, -10.5000, -10.7500, -11.0000, -11.2500,\n",
      "        -11.5000, -11.7500, -12.0000, -12.2500, -12.5000, -12.7500, -13.0000,\n",
      "        -13.2500, -13.5000, -13.7500, -14.0000, -14.2500, -14.5000, -14.7500,\n",
      "        -15.0000, -15.2500, -15.5000, -15.7500, -16.0000, -16.2500, -16.5000,\n",
      "        -16.7500, -17.0000, -17.2500, -17.5000, -17.7500, -18.0000, -18.2500,\n",
      "        -18.5000, -18.7500, -19.0000, -19.2500, -19.5000, -19.7500, -20.0000,\n",
      "        -20.2500, -20.5000, -20.7500, -21.0000, -21.2500, -21.5000, -21.7500,\n",
      "        -22.0000, -22.2500, -22.5000, -22.7500, -23.0000, -23.2500, -23.5000,\n",
      "        -23.7500, -24.0000, -24.2500, -24.5000, -24.7500, -25.0000, -25.2500,\n",
      "        -25.5000, -25.7500, -26.0000, -26.2500, -26.5000, -26.7500, -27.0000,\n",
      "        -27.2500, -27.5000, -27.7500, -28.0000, -28.2500, -28.5000, -28.7500,\n",
      "        -29.0000, -29.2500, -29.5000, -29.7500, -30.0000, -30.2500, -30.5000,\n",
      "        -30.7500, -31.0000, -31.2500, -31.5000, -31.7500, -32.0000, -32.2500,\n",
      "        -32.5000, -32.7500, -33.0000, -33.2500, -33.5000, -33.7500, -34.0000,\n",
      "        -34.2500, -34.5000, -34.7500, -35.0000, -35.2500, -35.5000, -35.7500,\n",
      "        -36.0000, -36.2500, -36.5000, -36.7500, -37.0000, -37.2500, -37.5000,\n",
      "        -37.7500, -38.0000, -38.2500, -38.5000, -38.7500, -39.0000, -39.2500,\n",
      "        -39.5000, -39.7500, -40.0000, -40.2500, -40.5000, -40.7500, -41.0000,\n",
      "        -41.2500, -41.5000, -41.7500, -42.0000, -42.2500, -42.5000, -42.7500,\n",
      "        -43.0000, -43.2500, -43.5000, -43.7500, -44.0000, -44.2500, -44.5000,\n",
      "        -44.7500, -45.0000, -45.2500, -45.5000, -45.7500, -46.0000, -46.2500,\n",
      "        -46.5000, -46.7500, -47.0000, -47.2500, -47.5000, -47.7500, -48.0000,\n",
      "        -48.2500, -48.5000, -48.7500, -49.0000, -49.2500, -49.5000, -49.7500,\n",
      "        -50.0000, -50.2500, -50.5000, -50.7500, -51.0000, -51.2500, -51.5000,\n",
      "        -51.7500, -52.0000, -52.2500, -52.5000, -52.7500, -53.0000, -53.2500,\n",
      "        -53.5000, -53.7500, -54.0000, -54.2500, -54.5000, -54.7500, -55.0000,\n",
      "        -55.2500, -55.5000, -55.7500, -56.0000, -56.2500, -56.5000, -56.7500,\n",
      "        -57.0000, -57.2500, -57.5000, -57.7500, -58.0000, -58.2500, -58.5000,\n",
      "        -58.7500, -59.0000, -59.2500, -59.5000, -59.7500, -60.0000, -60.2500,\n",
      "        -60.5000, -60.7500, -61.0000, -61.2500, -61.5000, -61.7500, -62.0000,\n",
      "        -62.2500, -62.5000, -62.7500, -63.0000, -63.2500, -63.5000, -63.7500,\n",
      "        -64.0000, -64.2500, -64.5000, -64.7500, -65.0000, -65.2500, -65.5000,\n",
      "        -65.7500, -66.0000, -66.2500, -66.5000, -66.7500, -67.0000, -67.2500,\n",
      "        -67.5000, -67.7500, -68.0000, -68.2500, -68.5000, -68.7500, -69.0000,\n",
      "        -69.2500, -69.5000, -69.7500, -70.0000, -70.2500, -70.5000, -70.7500,\n",
      "        -71.0000, -71.2500, -71.5000, -71.7500, -72.0000, -72.2500, -72.5000,\n",
      "        -72.7500, -73.0000, -73.2500, -73.5000, -73.7500, -74.0000, -74.2500,\n",
      "        -74.5000, -74.7500, -75.0000, -75.2500, -75.5000, -75.7500, -76.0000,\n",
      "        -76.2500, -76.5000, -76.7500, -77.0000, -77.2500, -77.5000, -77.7500,\n",
      "        -78.0000, -78.2500, -78.5000, -78.7500, -79.0000, -79.2500, -79.5000,\n",
      "        -79.7500, -80.0000, -80.2500, -80.5000, -80.7500, -81.0000, -81.2500,\n",
      "        -81.5000, -81.7500, -82.0000, -82.2500, -82.5000, -82.7500, -83.0000,\n",
      "        -83.2500, -83.5000, -83.7500, -84.0000, -84.2500, -84.5000, -84.7500,\n",
      "        -85.0000, -85.2500, -85.5000, -85.7500, -86.0000, -86.2500, -86.5000,\n",
      "        -86.7500, -87.0000, -87.2500, -87.5000, -87.7500, -88.0000, -88.2500,\n",
      "        -88.5000, -88.7500, -89.0000, -89.2500, -89.5000, -89.7500],\n",
      "       device='cuda:0'), lon=tensor([0.0000e+00, 2.5000e-01, 5.0000e-01,  ..., 3.5925e+02, 3.5950e+02,\n",
      "        3.5975e+02], device='cuda:0'), time=(datetime.datetime(2015, 1, 4, 12, 0),), atmos_levels=(1000, 925, 850, 700, 600, 500, 400, 300, 250, 200, 100, 50), rollout_step=1))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/torch/autograd/graph.py:825: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/MHA.cpp:674.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computed first batch\n",
      "Batch(surf_vars={'2t': tensor([[[[246.5566, 249.0670, 248.9123,  ..., 248.7042, 248.8638, 246.1772],\n",
      "          [254.5370, 254.2614, 253.9191,  ..., 253.9032, 253.9358, 248.8828],\n",
      "          [250.2312, 251.7356, 247.2331,  ..., 251.6860, 247.5735, 246.9567],\n",
      "          ...,\n",
      "          [245.6753, 245.1755, 245.6828,  ..., 246.0768, 246.3116, 246.4983],\n",
      "          [247.3932, 246.8410, 247.1264,  ..., 246.4315, 246.4003, 246.4061],\n",
      "          [245.2274, 245.0392, 245.4596,  ..., 246.7330, 246.5004, 246.3099]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>), '10u': tensor([[[[  2.6317,   2.3470,   0.6036,  ...,   1.8345,   0.3006,   2.6186],\n",
      "          [ -8.5061,  -7.9438, -10.3034,  ...,  -7.3911,  -9.7658,  -7.0160],\n",
      "          [ -5.3238,  -5.0784, -10.9660,  ...,  -4.7888, -10.6551,  -8.6438],\n",
      "          ...,\n",
      "          [ -4.4275,  -4.4794,  -4.4256,  ...,  -3.8151,  -3.9774,  -4.1757],\n",
      "          [ -3.8242,  -3.6514,  -3.3166,  ...,  -4.1819,  -4.0720,  -4.0007],\n",
      "          [ -2.8588,  -3.0151,  -3.1744,  ...,  -3.3562,  -3.2996,  -3.2045]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>), '10v': tensor([[[[-4.7491, -1.6176, -0.7612,  ..., -1.4165, -0.5728,  0.2893],\n",
      "          [-2.8881, -2.2159, -1.9689,  ..., -2.2295, -1.8949,  2.3415],\n",
      "          [-0.4747, -1.8935, -1.7017,  ..., -2.1070, -1.8317, -1.0392],\n",
      "          ...,\n",
      "          [-1.6352, -1.9366, -1.8378,  ..., -1.3894, -1.5044, -1.5757],\n",
      "          [-2.3182, -2.2129, -1.9340,  ..., -2.1369, -2.0058, -1.8792],\n",
      "          [-2.4786, -2.5525, -2.5754,  ..., -2.8270, -2.7938, -2.6295]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>), 'msl': tensor([[[[102834.5391, 102837.0781, 103025.3281,  ..., 102846.4766,\n",
      "           103030.6172, 103434.9531],\n",
      "          [102781.9531, 103022.9141, 103044.7422,  ..., 103046.5781,\n",
      "           103074.0078, 103226.7031],\n",
      "          [103082.3125, 102720.5156, 102909.7344,  ..., 102749.7734,\n",
      "           102940.0469, 103221.5156],\n",
      "          ...,\n",
      "          [100387.4609, 100432.2578, 100446.0938,  ..., 100324.0000,\n",
      "           100320.5859, 100315.9219],\n",
      "          [100378.5547, 100403.3906, 100385.3594,  ..., 100296.4062,\n",
      "           100302.9375, 100300.4453],\n",
      "          [100358.1797, 100340.3281, 100283.6875,  ..., 100229.9219,\n",
      "           100226.4141, 100236.1406]]]], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>), 'fire': tensor([[[[ 1.4000,  0.4946,  0.4597,  ...,  0.0145,  0.6163, -1.2973],\n",
      "          [-2.9834,  1.6034,  0.8447,  ...,  1.6434,  1.2762,  1.4595],\n",
      "          [-0.5256, -1.6139, -0.6010,  ..., -1.6112, -0.5811,  1.9214],\n",
      "          ...,\n",
      "          [-0.5346, -0.7558, -0.2672,  ..., -0.9423,  0.0912,  0.7919],\n",
      "          [ 0.0804, -0.2350, -0.6094,  ...,  0.3667, -0.5326, -0.1285],\n",
      "          [ 0.8870, -0.7816,  1.4725,  ..., -1.0411,  0.5906, -0.2948]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>), 'lst': tensor([[[[ 6144.7876, 11625.5176, 13603.5322,  ..., 14394.2168,\n",
      "           10728.5205, -2008.1045],\n",
      "          [29104.3164,  5814.5918, 36189.8398,  ...,  6440.6968,\n",
      "           38131.1484, 17418.6641],\n",
      "          [-6850.8525, 34185.3281, 15204.8008,  ..., 31436.2031,\n",
      "           15227.7939,  2587.2207],\n",
      "          ...,\n",
      "          [15447.8086, 13563.4932, 14566.8828,  ..., 13585.0703,\n",
      "           17283.3516, 11065.9502],\n",
      "          [12127.2354, 18459.2168, 12575.8447,  ..., 14825.0049,\n",
      "           10619.8564,  2944.5127],\n",
      "          [19972.0664,  7581.4106,  9182.0479,  ...,  7052.6211,\n",
      "            6492.8052,  8230.0654]]]], device='cuda:0', grad_fn=<AddBackward0>)}, static_vars={'z': tensor([[ 3.2275e-01,  3.2275e-01,  3.2275e-01,  ...,  3.2275e-01,\n",
      "          3.2275e-01,  3.2275e-01],\n",
      "        [ 4.6157e+00,  4.6040e+00,  4.5884e+00,  ...,  4.7915e+00,\n",
      "          4.7329e+00,  4.6743e+00],\n",
      "        [-1.7710e+00, -1.7163e+00, -1.7085e+00,  ..., -1.5093e+00,\n",
      "         -1.6226e+00, -1.7124e+00],\n",
      "        ...,\n",
      "        [ 2.6638e+04,  2.6642e+04,  2.6647e+04,  ...,  2.6627e+04,\n",
      "          2.6631e+04,  2.6634e+04],\n",
      "        [ 2.6958e+04,  2.6961e+04,  2.6965e+04,  ...,  2.6950e+04,\n",
      "          2.6953e+04,  2.6956e+04],\n",
      "        [ 2.7163e+04,  2.7165e+04,  2.7167e+04,  ...,  2.7159e+04,\n",
      "          2.7160e+04,  2.7162e+04]], device='cuda:0'), 'slt': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0'), 'lsm': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0')}, atmos_vars={'t': tensor([[[[[252.3774, 252.2945, 252.3273,  ..., 251.6500, 251.7189,\n",
      "            251.7875],\n",
      "           [252.6422, 252.6296, 252.6951,  ..., 252.1444, 252.2133,\n",
      "            252.2300],\n",
      "           [253.2570, 253.2582, 253.3021,  ..., 253.0252, 253.0634,\n",
      "            253.1299],\n",
      "           ...,\n",
      "           [264.6792, 264.4735, 264.3450,  ..., 264.9097, 264.8164,\n",
      "            264.7161],\n",
      "           [265.5323, 265.3532, 265.2997,  ..., 265.6867, 265.6630,\n",
      "            265.7100],\n",
      "           [265.7731, 265.6615, 265.6553,  ..., 266.0635, 266.1049,\n",
      "            266.2934]],\n",
      "\n",
      "          [[253.0310, 252.9886, 253.0017,  ..., 252.8642, 252.8846,\n",
      "            252.8692],\n",
      "           [253.1220, 253.1065, 253.1389,  ..., 253.0272, 253.0603,\n",
      "            252.9821],\n",
      "           [253.4720, 253.4563, 253.4294,  ..., 253.3661, 253.3392,\n",
      "            253.3361],\n",
      "           ...,\n",
      "           [261.5435, 261.5309, 261.5424,  ..., 261.6942, 261.7292,\n",
      "            261.6651],\n",
      "           [262.1307, 262.1441, 262.1745,  ..., 262.3028, 262.3480,\n",
      "            262.3600],\n",
      "           [262.5142, 262.5372, 262.5657,  ..., 262.7040, 262.7420,\n",
      "            262.8462]],\n",
      "\n",
      "          [[252.6878, 252.6485, 252.6337,  ..., 252.4814, 252.4725,\n",
      "            252.4221],\n",
      "           [252.5008, 252.4841, 252.5257,  ..., 252.3455, 252.3915,\n",
      "            252.2821],\n",
      "           [252.3612, 252.3422, 252.3424,  ..., 252.2279, 252.2338,\n",
      "            252.2314],\n",
      "           ...,\n",
      "           [266.7706, 266.7178, 266.7315,  ..., 266.8543, 266.8914,\n",
      "            266.9150],\n",
      "           [267.1185, 267.0588, 267.0525,  ..., 267.0464, 267.0746,\n",
      "            267.0926],\n",
      "           [267.6879, 267.6068, 267.5368,  ..., 267.4385, 267.4129,\n",
      "            267.4601]],\n",
      "\n",
      "          ...,\n",
      "\n",
      "          [[216.4057, 216.4100, 216.4079,  ..., 216.4233, 216.4165,\n",
      "            216.4322],\n",
      "           [216.1671, 216.1650, 216.1785,  ..., 216.2175, 216.2257,\n",
      "            216.2397],\n",
      "           [215.9821, 215.9820, 216.0063,  ..., 216.0350, 216.0546,\n",
      "            216.0631],\n",
      "           ...,\n",
      "           [221.5344, 221.5020, 221.4890,  ..., 221.5345, 221.5337,\n",
      "            221.5358],\n",
      "           [221.7263, 221.7289, 221.7489,  ..., 221.7517, 221.7868,\n",
      "            221.7984],\n",
      "           [222.1796, 222.2359, 222.2815,  ..., 222.2289, 222.2936,\n",
      "            222.3329]],\n",
      "\n",
      "          [[216.2871, 216.2937, 216.2982,  ..., 216.1542, 216.1616,\n",
      "            216.1795],\n",
      "           [216.1660, 216.1373, 216.1534,  ..., 215.9852, 216.0001,\n",
      "            216.0127],\n",
      "           [215.9894, 215.9690, 215.9823,  ..., 215.8046, 215.8162,\n",
      "            215.7923],\n",
      "           ...,\n",
      "           [224.9697, 224.9489, 224.9990,  ..., 224.5139, 224.5565,\n",
      "            224.5749],\n",
      "           [224.7167, 224.6527, 224.6613,  ..., 224.2941, 224.3117,\n",
      "            224.4095],\n",
      "           [224.5340, 224.4138, 224.3934,  ..., 224.1434, 224.1301,\n",
      "            224.2703]],\n",
      "\n",
      "          [[216.5611, 216.4858, 216.4404,  ..., 216.3982, 216.3571,\n",
      "            216.3592],\n",
      "           [216.4693, 216.3705, 216.3330,  ..., 216.3323, 216.2964,\n",
      "            216.3005],\n",
      "           [216.3665, 216.3008, 216.2719,  ..., 216.3131, 216.2844,\n",
      "            216.2524],\n",
      "           ...,\n",
      "           [221.2417, 221.2159, 221.2702,  ..., 220.4533, 220.4974,\n",
      "            220.5000],\n",
      "           [221.0808, 221.0793, 221.0918,  ..., 220.3439, 220.3423,\n",
      "            220.3206],\n",
      "           [220.9647, 220.9679, 220.9572,  ..., 220.2793, 220.2506,\n",
      "            220.2734]]]]], device='cuda:0', grad_fn=<AddBackward0>), 'u': tensor([[[[[ 4.3017e-01,  4.6180e-01,  5.0104e-01,  ...,  5.4468e-01,\n",
      "             5.8731e-01,  5.9429e-01],\n",
      "           [-1.0283e+00, -1.0520e+00, -1.0133e+00,  ..., -8.5369e-01,\n",
      "            -8.0266e-01, -7.3195e-01],\n",
      "           [-2.8945e+00, -2.9496e+00, -2.9383e+00,  ..., -2.5610e+00,\n",
      "            -2.5158e+00, -2.4275e+00],\n",
      "           ...,\n",
      "           [-4.9328e+00, -4.8711e+00, -4.7442e+00,  ..., -4.6491e+00,\n",
      "            -4.5422e+00, -4.4725e+00],\n",
      "           [-4.7713e+00, -4.7595e+00, -4.6453e+00,  ..., -4.6799e+00,\n",
      "            -4.5253e+00, -4.4074e+00],\n",
      "           [-4.2452e+00, -4.2335e+00, -4.1742e+00,  ..., -4.3166e+00,\n",
      "            -4.2047e+00, -4.1137e+00]],\n",
      "\n",
      "          [[ 2.4542e-01,  2.7229e-01,  3.1023e-01,  ...,  1.9747e-01,\n",
      "             2.3825e-01,  2.6239e-01],\n",
      "           [ 7.9175e-01,  7.9159e-01,  7.9335e-01,  ...,  4.4407e-01,\n",
      "             4.4473e-01,  4.5691e-01],\n",
      "           [-7.1103e-01, -7.2071e-01, -7.5226e-01,  ..., -8.6255e-01,\n",
      "            -8.9632e-01, -9.1762e-01],\n",
      "           ...,\n",
      "           [-5.5660e+00, -5.5702e+00, -5.5950e+00,  ..., -4.9691e+00,\n",
      "            -4.9897e+00, -4.9974e+00],\n",
      "           [-5.0617e+00, -5.1195e+00, -5.1483e+00,  ..., -4.7433e+00,\n",
      "            -4.7417e+00, -4.7568e+00],\n",
      "           [-4.5692e+00, -4.6393e+00, -4.6982e+00,  ..., -4.3095e+00,\n",
      "            -4.3115e+00, -4.2995e+00]],\n",
      "\n",
      "          [[ 6.4684e-01,  6.3157e-01,  6.1190e-01,  ...,  4.0365e-01,\n",
      "             3.9706e-01,  4.1428e-01],\n",
      "           [ 1.6508e+00,  1.6410e+00,  1.6126e+00,  ...,  1.3010e+00,\n",
      "             1.2873e+00,  1.2942e+00],\n",
      "           [ 4.0517e-01,  4.0462e-01,  3.7443e-01,  ...,  2.2109e-01,\n",
      "             2.0396e-01,  1.7399e-01],\n",
      "           ...,\n",
      "           [-4.8653e+00, -4.9890e+00, -5.0665e+00,  ..., -4.5825e+00,\n",
      "            -4.7076e+00, -4.7425e+00],\n",
      "           [-4.4291e+00, -4.5512e+00, -4.5971e+00,  ..., -4.4873e+00,\n",
      "            -4.5379e+00, -4.5651e+00],\n",
      "           [-3.9415e+00, -4.0108e+00, -4.0346e+00,  ..., -4.0594e+00,\n",
      "            -4.0864e+00, -4.0953e+00]],\n",
      "\n",
      "          ...,\n",
      "\n",
      "          [[ 4.7333e-01,  4.4951e-01,  4.7512e-01,  ..., -1.1837e-01,\n",
      "            -9.3856e-02, -1.4098e-02],\n",
      "           [-1.1629e+01, -1.1567e+01, -1.1560e+01,  ..., -1.1215e+01,\n",
      "            -1.1225e+01, -1.1247e+01],\n",
      "           [-1.0943e+01, -1.0942e+01, -1.0852e+01,  ..., -1.0465e+01,\n",
      "            -1.0390e+01, -1.0443e+01],\n",
      "           ...,\n",
      "           [-1.8769e+00, -2.0882e+00, -2.2592e+00,  ..., -1.8874e+00,\n",
      "            -2.0998e+00, -2.2735e+00],\n",
      "           [-1.5519e+00, -1.6832e+00, -1.7080e+00,  ..., -1.4771e+00,\n",
      "            -1.5410e+00, -1.6838e+00],\n",
      "           [-1.1935e+00, -1.2990e+00, -1.3276e+00,  ..., -1.0858e+00,\n",
      "            -1.1474e+00, -1.2900e+00]],\n",
      "\n",
      "          [[ 6.3765e-01,  6.2490e-01,  5.9840e-01,  ...,  6.6461e-01,\n",
      "             6.3113e-01,  6.5657e-01],\n",
      "           [-1.9643e+01, -1.9663e+01, -1.9657e+01,  ..., -1.9683e+01,\n",
      "            -1.9675e+01, -1.9543e+01],\n",
      "           [-1.8820e+01, -1.8976e+01, -1.8949e+01,  ..., -1.8900e+01,\n",
      "            -1.8861e+01, -1.8724e+01],\n",
      "           ...,\n",
      "           [-3.0949e+00, -3.4688e+00, -3.6862e+00,  ..., -2.6615e+00,\n",
      "            -2.8948e+00, -2.9937e+00],\n",
      "           [-3.6680e+00, -4.0176e+00, -4.0031e+00,  ..., -3.2543e+00,\n",
      "            -3.2331e+00, -3.1285e+00],\n",
      "           [-4.1333e+00, -4.2212e+00, -4.0261e+00,  ..., -3.5528e+00,\n",
      "            -3.3449e+00, -2.9863e+00]],\n",
      "\n",
      "          [[ 4.8433e-01,  6.3909e-01,  7.8113e-01,  ...,  1.1685e+00,\n",
      "             1.2871e+00,  1.4845e+00],\n",
      "           [-2.6364e+01, -2.6248e+01, -2.6138e+01,  ..., -2.5930e+01,\n",
      "            -2.5858e+01, -2.5699e+01],\n",
      "           [-2.5717e+01, -2.5675e+01, -2.5558e+01,  ..., -2.5376e+01,\n",
      "            -2.5283e+01, -2.5116e+01],\n",
      "           ...,\n",
      "           [-3.6172e+00, -4.1453e+00, -4.7117e+00,  ..., -3.3122e+00,\n",
      "            -3.8711e+00, -4.2276e+00],\n",
      "           [-4.8291e+00, -5.1559e+00, -5.2679e+00,  ..., -4.5879e+00,\n",
      "            -4.6818e+00, -4.6103e+00],\n",
      "           [-5.5330e+00, -5.6302e+00, -5.5489e+00,  ..., -5.0632e+00,\n",
      "            -4.9161e+00, -4.5610e+00]]]]], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>), 'v': tensor([[[[[-2.8347e-01, -1.5753e-01, -8.1578e-02,  ...,  7.8951e-02,\n",
      "             1.4987e-01,  1.9298e-01],\n",
      "           [ 3.3479e-01,  4.9477e-01,  5.8367e-01,  ...,  9.6620e-02,\n",
      "             1.4837e-01,  1.5331e-01],\n",
      "           [ 1.1867e-01,  2.9251e-01,  3.9293e-01,  ..., -1.2634e-01,\n",
      "            -4.4183e-02, -3.5591e-02],\n",
      "           ...,\n",
      "           [-9.0485e-01, -1.0984e+00, -1.3564e+00,  ..., -1.1245e+00,\n",
      "            -1.3395e+00, -1.5550e+00],\n",
      "           [-1.2449e+00, -1.4377e+00, -1.6949e+00,  ..., -1.5959e+00,\n",
      "            -1.7710e+00, -1.9543e+00],\n",
      "           [-1.4841e+00, -1.6031e+00, -1.7602e+00,  ..., -2.0345e+00,\n",
      "            -2.1173e+00, -2.2440e+00]],\n",
      "\n",
      "          [[-1.6721e-01, -6.3922e-02, -7.4174e-02,  ...,  1.9314e-01,\n",
      "             1.7545e-01,  1.3430e-01],\n",
      "           [ 1.3391e+00,  1.4328e+00,  1.4372e+00,  ...,  1.5609e+00,\n",
      "             1.5489e+00,  1.5167e+00],\n",
      "           [ 1.0519e+00,  1.1409e+00,  1.1964e+00,  ...,  1.1936e+00,\n",
      "             1.2256e+00,  1.2551e+00],\n",
      "           ...,\n",
      "           [-1.2312e+00, -1.3108e+00, -1.3711e+00,  ..., -1.2614e+00,\n",
      "            -1.3023e+00, -1.3735e+00],\n",
      "           [-1.7032e+00, -1.7479e+00, -1.7531e+00,  ..., -1.7478e+00,\n",
      "            -1.7408e+00, -1.7809e+00],\n",
      "           [-2.2618e+00, -2.2768e+00, -2.2584e+00,  ..., -2.3061e+00,\n",
      "            -2.2900e+00, -2.3480e+00]],\n",
      "\n",
      "          [[ 2.0654e-01,  3.0968e-01,  3.6619e-01,  ...,  4.2716e-01,\n",
      "             4.8847e-01,  5.3580e-01],\n",
      "           [ 4.2146e-02,  1.5269e-01,  2.2348e-01,  ...,  2.0734e-01,\n",
      "             2.7340e-01,  3.2950e-01],\n",
      "           [-4.1887e-01, -3.0109e-01, -1.9554e-01,  ..., -2.7938e-01,\n",
      "            -1.8511e-01, -9.5417e-02],\n",
      "           ...,\n",
      "           [-1.2794e+00, -1.2687e+00, -1.2898e+00,  ..., -1.0860e+00,\n",
      "            -1.1062e+00, -1.1551e+00],\n",
      "           [-1.5803e+00, -1.5748e+00, -1.6130e+00,  ..., -1.5250e+00,\n",
      "            -1.5301e+00, -1.5872e+00],\n",
      "           [-2.0062e+00, -2.0160e+00, -2.0511e+00,  ..., -2.1791e+00,\n",
      "            -2.1592e+00, -2.1711e+00]],\n",
      "\n",
      "          ...,\n",
      "\n",
      "          [[ 2.8700e-01,  3.3414e-01,  3.9681e-01,  ...,  5.0618e-01,\n",
      "             5.5109e-01,  6.1807e-01],\n",
      "           [-5.8099e+00, -5.7223e+00, -5.6051e+00,  ..., -5.7784e+00,\n",
      "            -5.6687e+00, -5.5769e+00],\n",
      "           [-5.5289e+00, -5.4870e+00, -5.3846e+00,  ..., -5.4677e+00,\n",
      "            -5.3648e+00, -5.2787e+00],\n",
      "           ...,\n",
      "           [ 2.5264e-01,  1.7639e-01,  1.5000e-01,  ...,  3.6550e-01,\n",
      "             3.0378e-01,  1.4558e-01],\n",
      "           [ 8.0600e-02, -1.0836e-01, -2.3506e-01,  ...,  6.0837e-02,\n",
      "            -8.8519e-02, -2.9387e-01],\n",
      "           [ 2.0282e-01, -9.1003e-02, -3.3489e-01,  ...,  5.8806e-04,\n",
      "            -2.5067e-01, -6.0203e-01]],\n",
      "\n",
      "          [[-3.8182e-01, -3.4774e-01, -3.5789e-01,  ..., -2.2505e-01,\n",
      "            -2.3280e-01, -2.2769e-01],\n",
      "           [-4.8043e+00, -4.6698e+00, -4.5615e+00,  ..., -5.1366e+00,\n",
      "            -5.0229e+00, -4.9048e+00],\n",
      "           [-5.1336e+00, -5.0353e+00, -4.9209e+00,  ..., -5.3482e+00,\n",
      "            -5.2304e+00, -5.1069e+00],\n",
      "           ...,\n",
      "           [-1.0067e-01, -2.9646e-01, -4.8640e-01,  ..., -4.0748e-02,\n",
      "            -1.8056e-01, -3.6679e-01],\n",
      "           [-5.2004e-01, -8.9640e-01, -1.1648e+00,  ..., -6.8489e-01,\n",
      "            -9.0606e-01, -1.1765e+00],\n",
      "           [-7.8707e-01, -1.2401e+00, -1.6678e+00,  ..., -1.1071e+00,\n",
      "            -1.4897e+00, -1.8898e+00]],\n",
      "\n",
      "          [[-6.0376e-02, -2.9135e-02, -2.4746e-03,  ..., -1.1926e-01,\n",
      "            -1.2008e-01, -1.1856e-01],\n",
      "           [-6.0247e+00, -5.8965e+00, -5.7545e+00,  ..., -6.4905e+00,\n",
      "            -6.3714e+00, -6.2730e+00],\n",
      "           [-6.3754e+00, -6.2914e+00, -6.1592e+00,  ..., -6.7045e+00,\n",
      "            -6.5888e+00, -6.4969e+00],\n",
      "           ...,\n",
      "           [-3.9365e-01, -6.7556e-01, -9.2687e-01,  ..., -5.2899e-02,\n",
      "            -3.1294e-01, -6.3955e-01],\n",
      "           [-5.9159e-01, -9.2470e-01, -1.1911e+00,  ..., -3.3063e-01,\n",
      "            -5.7315e-01, -8.0275e-01],\n",
      "           [-6.6085e-01, -1.0224e+00, -1.3868e+00,  ..., -4.2260e-01,\n",
      "            -7.3061e-01, -9.9722e-01]]]]], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>), 'q': tensor([[[[[4.1727e-04, 4.3268e-04, 4.4436e-04,  ..., 4.0706e-04,\n",
      "            4.2937e-04, 4.3444e-04],\n",
      "           [4.5428e-04, 4.5223e-04, 4.5291e-04,  ..., 4.2855e-04,\n",
      "            4.3856e-04, 4.5185e-04],\n",
      "           [4.5132e-04, 4.6096e-04, 4.4006e-04,  ..., 4.4358e-04,\n",
      "            4.2933e-04, 4.3515e-04],\n",
      "           ...,\n",
      "           [5.8003e-04, 6.0440e-04, 6.2450e-04,  ..., 5.5202e-04,\n",
      "            5.7687e-04, 5.8682e-04],\n",
      "           [5.1718e-04, 5.6651e-04, 5.9106e-04,  ..., 5.0149e-04,\n",
      "            5.2743e-04, 5.4696e-04],\n",
      "           [4.9438e-04, 5.2895e-04, 5.7211e-04,  ..., 4.7273e-04,\n",
      "            5.2210e-04, 5.4555e-04]],\n",
      "\n",
      "          [[3.4961e-04, 3.6575e-04, 3.8077e-04,  ..., 3.7184e-04,\n",
      "            3.8459e-04, 3.7652e-04],\n",
      "           [3.8745e-04, 3.8173e-04, 3.9055e-04,  ..., 3.9312e-04,\n",
      "            3.9871e-04, 4.0809e-04],\n",
      "           [4.0281e-04, 4.1162e-04, 3.9650e-04,  ..., 4.2583e-04,\n",
      "            4.0616e-04, 4.0847e-04],\n",
      "           ...,\n",
      "           [4.7341e-04, 4.6520e-04, 4.6721e-04,  ..., 4.2336e-04,\n",
      "            4.2291e-04, 4.2944e-04],\n",
      "           [4.1314e-04, 4.4613e-04, 4.4577e-04,  ..., 4.0005e-04,\n",
      "            3.8985e-04, 3.9282e-04],\n",
      "           [3.9036e-04, 4.0972e-04, 4.3226e-04,  ..., 3.6311e-04,\n",
      "            3.7575e-04, 3.7482e-04]],\n",
      "\n",
      "          [[1.3012e-04, 1.3147e-04, 1.3159e-04,  ..., 1.4627e-04,\n",
      "            1.4493e-04, 1.3011e-04],\n",
      "           [1.5321e-04, 1.4213e-04, 1.3700e-04,  ..., 1.5685e-04,\n",
      "            1.5187e-04, 1.5257e-04],\n",
      "           [1.4556e-04, 1.5198e-04, 1.3527e-04,  ..., 1.7239e-04,\n",
      "            1.5632e-04, 1.4731e-04],\n",
      "           ...,\n",
      "           [6.8900e-04, 6.9676e-04, 6.8343e-04,  ..., 6.7602e-04,\n",
      "            6.6229e-04, 6.4743e-04],\n",
      "           [6.8573e-04, 7.1557e-04, 7.0299e-04,  ..., 6.9579e-04,\n",
      "            6.7834e-04, 6.7191e-04],\n",
      "           [6.6568e-04, 7.0730e-04, 7.3201e-04,  ..., 7.0963e-04,\n",
      "            7.2813e-04, 7.3705e-04]],\n",
      "\n",
      "          ...,\n",
      "\n",
      "          [[3.0883e-06, 3.2500e-06, 3.3774e-06,  ..., 3.2895e-06,\n",
      "            3.3877e-06, 3.3552e-06],\n",
      "           [3.1963e-06, 3.2198e-06, 3.2748e-06,  ..., 3.2472e-06,\n",
      "            3.2824e-06, 3.1944e-06],\n",
      "           [3.1014e-06, 3.0931e-06, 3.0543e-06,  ..., 3.1476e-06,\n",
      "            3.0983e-06, 3.1477e-06],\n",
      "           ...,\n",
      "           [2.9104e-06, 2.8069e-06, 2.8602e-06,  ..., 2.8640e-06,\n",
      "            2.9394e-06, 2.9572e-06],\n",
      "           [2.5730e-06, 2.4053e-06, 2.4039e-06,  ..., 2.4047e-06,\n",
      "            2.4414e-06, 2.5883e-06],\n",
      "           [2.7058e-06, 2.4620e-06, 2.3218e-06,  ..., 2.4454e-06,\n",
      "            2.3389e-06, 2.3667e-06]],\n",
      "\n",
      "          [[2.8199e-06, 2.8191e-06, 2.8164e-06,  ..., 2.8159e-06,\n",
      "            2.8136e-06, 2.8114e-06],\n",
      "           [2.8077e-06, 2.8062e-06, 2.8059e-06,  ..., 2.8010e-06,\n",
      "            2.8007e-06, 2.7993e-06],\n",
      "           [2.7918e-06, 2.7904e-06, 2.7915e-06,  ..., 2.7891e-06,\n",
      "            2.7893e-06, 2.7885e-06],\n",
      "           ...,\n",
      "           [2.4817e-06, 2.4904e-06, 2.4893e-06,  ..., 2.4903e-06,\n",
      "            2.4897e-06, 2.4811e-06],\n",
      "           [2.4411e-06, 2.4524e-06, 2.4587e-06,  ..., 2.4557e-06,\n",
      "            2.4627e-06, 2.4622e-06],\n",
      "           [2.4199e-06, 2.4311e-06, 2.4400e-06,  ..., 2.4395e-06,\n",
      "            2.4494e-06, 2.4437e-06]],\n",
      "\n",
      "          [[2.9674e-06, 2.9682e-06, 2.9688e-06,  ..., 2.9735e-06,\n",
      "            2.9727e-06, 2.9730e-06],\n",
      "           [2.9738e-06, 2.9740e-06, 2.9748e-06,  ..., 2.9795e-06,\n",
      "            2.9787e-06, 2.9776e-06],\n",
      "           [2.9763e-06, 2.9775e-06, 2.9785e-06,  ..., 2.9841e-06,\n",
      "            2.9834e-06, 2.9830e-06],\n",
      "           ...,\n",
      "           [2.5677e-06, 2.5788e-06, 2.5841e-06,  ..., 2.5821e-06,\n",
      "            2.5877e-06, 2.5814e-06],\n",
      "           [2.5430e-06, 2.5562e-06, 2.5658e-06,  ..., 2.5583e-06,\n",
      "            2.5686e-06, 2.5681e-06],\n",
      "           [2.5349e-06, 2.5400e-06, 2.5419e-06,  ..., 2.5417e-06,\n",
      "            2.5441e-06, 2.5389e-06]]]]], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>), 'z': tensor([[[[[1.6842e+03, 1.6858e+03, 1.6863e+03,  ..., 1.7088e+03,\n",
      "            1.7080e+03, 1.7051e+03],\n",
      "           [1.6969e+03, 1.6985e+03, 1.6983e+03,  ..., 1.7122e+03,\n",
      "            1.7121e+03, 1.7110e+03],\n",
      "           [1.6950e+03, 1.6951e+03, 1.6979e+03,  ..., 1.7043e+03,\n",
      "            1.7082e+03, 1.7073e+03],\n",
      "           ...,\n",
      "           [1.3454e+02, 1.3720e+02, 1.4128e+02,  ..., 1.2507e+02,\n",
      "            1.2934e+02, 1.3093e+02],\n",
      "           [1.0452e+02, 1.0448e+02, 1.0150e+02,  ..., 8.6210e+01,\n",
      "            8.2102e+01, 7.9669e+01],\n",
      "           [8.8417e+01, 8.3929e+01, 7.6226e+01,  ..., 6.2858e+01,\n",
      "            5.2565e+01, 4.4564e+01]],\n",
      "\n",
      "          [[7.3445e+03, 7.3456e+03, 7.3469e+03,  ..., 7.3625e+03,\n",
      "            7.3635e+03, 7.3638e+03],\n",
      "           [7.3624e+03, 7.3627e+03, 7.3628e+03,  ..., 7.3750e+03,\n",
      "            7.3750e+03, 7.3748e+03],\n",
      "           [7.3732e+03, 7.3720e+03, 7.3735e+03,  ..., 7.3811e+03,\n",
      "            7.3827e+03, 7.3829e+03],\n",
      "           ...,\n",
      "           [6.0651e+03, 6.0653e+03, 6.0697e+03,  ..., 6.0607e+03,\n",
      "            6.0645e+03, 6.0670e+03],\n",
      "           [6.0459e+03, 6.0488e+03, 6.0520e+03,  ..., 6.0448e+03,\n",
      "            6.0470e+03, 6.0499e+03],\n",
      "           [6.0312e+03, 6.0347e+03, 6.0387e+03,  ..., 6.0357e+03,\n",
      "            6.0391e+03, 6.0387e+03]],\n",
      "\n",
      "          [[1.3496e+04, 1.3498e+04, 1.3498e+04,  ..., 1.3514e+04,\n",
      "            1.3514e+04, 1.3514e+04],\n",
      "           [1.3515e+04, 1.3516e+04, 1.3516e+04,  ..., 1.3527e+04,\n",
      "            1.3527e+04, 1.3527e+04],\n",
      "           [1.3527e+04, 1.3526e+04, 1.3527e+04,  ..., 1.3533e+04,\n",
      "            1.3535e+04, 1.3534e+04],\n",
      "           ...,\n",
      "           [1.2510e+04, 1.2510e+04, 1.2518e+04,  ..., 1.2504e+04,\n",
      "            1.2510e+04, 1.2515e+04],\n",
      "           [1.2511e+04, 1.2516e+04, 1.2518e+04,  ..., 1.2511e+04,\n",
      "            1.2513e+04, 1.2519e+04],\n",
      "           [1.2506e+04, 1.2512e+04, 1.2518e+04,  ..., 1.2508e+04,\n",
      "            1.2513e+04, 1.2515e+04]],\n",
      "\n",
      "          ...,\n",
      "\n",
      "          [[1.0829e+05, 1.0830e+05, 1.0829e+05,  ..., 1.0830e+05,\n",
      "            1.0829e+05, 1.0830e+05],\n",
      "           [1.0821e+05, 1.0821e+05, 1.0822e+05,  ..., 1.0822e+05,\n",
      "            1.0822e+05, 1.0826e+05],\n",
      "           [1.0820e+05, 1.0818e+05, 1.0820e+05,  ..., 1.0819e+05,\n",
      "            1.0821e+05, 1.0819e+05],\n",
      "           ...,\n",
      "           [1.0897e+05, 1.0897e+05, 1.0898e+05,  ..., 1.0899e+05,\n",
      "            1.0900e+05, 1.0902e+05],\n",
      "           [1.0898e+05, 1.0899e+05, 1.0900e+05,  ..., 1.0901e+05,\n",
      "            1.0902e+05, 1.0903e+05],\n",
      "           [1.0898e+05, 1.0899e+05, 1.0900e+05,  ..., 1.0901e+05,\n",
      "            1.0902e+05, 1.0902e+05]],\n",
      "\n",
      "          [[1.5133e+05, 1.5134e+05, 1.5133e+05,  ..., 1.5134e+05,\n",
      "            1.5134e+05, 1.5135e+05],\n",
      "           [1.5122e+05, 1.5122e+05, 1.5123e+05,  ..., 1.5123e+05,\n",
      "            1.5124e+05, 1.5127e+05],\n",
      "           [1.5116e+05, 1.5115e+05, 1.5117e+05,  ..., 1.5117e+05,\n",
      "            1.5118e+05, 1.5117e+05],\n",
      "           ...,\n",
      "           [1.5390e+05, 1.5391e+05, 1.5393e+05,  ..., 1.5388e+05,\n",
      "            1.5390e+05, 1.5390e+05],\n",
      "           [1.5389e+05, 1.5392e+05, 1.5394e+05,  ..., 1.5389e+05,\n",
      "            1.5391e+05, 1.5393e+05],\n",
      "           [1.5389e+05, 1.5392e+05, 1.5394e+05,  ..., 1.5389e+05,\n",
      "            1.5390e+05, 1.5391e+05]],\n",
      "\n",
      "          [[1.9408e+05, 1.9407e+05, 1.9407e+05,  ..., 1.9408e+05,\n",
      "            1.9407e+05, 1.9409e+05],\n",
      "           [1.9395e+05, 1.9394e+05, 1.9395e+05,  ..., 1.9395e+05,\n",
      "            1.9395e+05, 1.9397e+05],\n",
      "           [1.9385e+05, 1.9385e+05, 1.9385e+05,  ..., 1.9387e+05,\n",
      "            1.9387e+05, 1.9387e+05],\n",
      "           ...,\n",
      "           [1.9688e+05, 1.9689e+05, 1.9691e+05,  ..., 1.9680e+05,\n",
      "            1.9682e+05, 1.9682e+05],\n",
      "           [1.9688e+05, 1.9692e+05, 1.9694e+05,  ..., 1.9682e+05,\n",
      "            1.9684e+05, 1.9687e+05],\n",
      "           [1.9688e+05, 1.9690e+05, 1.9693e+05,  ..., 1.9680e+05,\n",
      "            1.9683e+05, 1.9685e+05]]]]], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)}, metadata=Metadata(lat=tensor([ 90.0000,  89.7500,  89.5000,  89.2500,  89.0000,  88.7500,  88.5000,\n",
      "         88.2500,  88.0000,  87.7500,  87.5000,  87.2500,  87.0000,  86.7500,\n",
      "         86.5000,  86.2500,  86.0000,  85.7500,  85.5000,  85.2500,  85.0000,\n",
      "         84.7500,  84.5000,  84.2500,  84.0000,  83.7500,  83.5000,  83.2500,\n",
      "         83.0000,  82.7500,  82.5000,  82.2500,  82.0000,  81.7500,  81.5000,\n",
      "         81.2500,  81.0000,  80.7500,  80.5000,  80.2500,  80.0000,  79.7500,\n",
      "         79.5000,  79.2500,  79.0000,  78.7500,  78.5000,  78.2500,  78.0000,\n",
      "         77.7500,  77.5000,  77.2500,  77.0000,  76.7500,  76.5000,  76.2500,\n",
      "         76.0000,  75.7500,  75.5000,  75.2500,  75.0000,  74.7500,  74.5000,\n",
      "         74.2500,  74.0000,  73.7500,  73.5000,  73.2500,  73.0000,  72.7500,\n",
      "         72.5000,  72.2500,  72.0000,  71.7500,  71.5000,  71.2500,  71.0000,\n",
      "         70.7500,  70.5000,  70.2500,  70.0000,  69.7500,  69.5000,  69.2500,\n",
      "         69.0000,  68.7500,  68.5000,  68.2500,  68.0000,  67.7500,  67.5000,\n",
      "         67.2500,  67.0000,  66.7500,  66.5000,  66.2500,  66.0000,  65.7500,\n",
      "         65.5000,  65.2500,  65.0000,  64.7500,  64.5000,  64.2500,  64.0000,\n",
      "         63.7500,  63.5000,  63.2500,  63.0000,  62.7500,  62.5000,  62.2500,\n",
      "         62.0000,  61.7500,  61.5000,  61.2500,  61.0000,  60.7500,  60.5000,\n",
      "         60.2500,  60.0000,  59.7500,  59.5000,  59.2500,  59.0000,  58.7500,\n",
      "         58.5000,  58.2500,  58.0000,  57.7500,  57.5000,  57.2500,  57.0000,\n",
      "         56.7500,  56.5000,  56.2500,  56.0000,  55.7500,  55.5000,  55.2500,\n",
      "         55.0000,  54.7500,  54.5000,  54.2500,  54.0000,  53.7500,  53.5000,\n",
      "         53.2500,  53.0000,  52.7500,  52.5000,  52.2500,  52.0000,  51.7500,\n",
      "         51.5000,  51.2500,  51.0000,  50.7500,  50.5000,  50.2500,  50.0000,\n",
      "         49.7500,  49.5000,  49.2500,  49.0000,  48.7500,  48.5000,  48.2500,\n",
      "         48.0000,  47.7500,  47.5000,  47.2500,  47.0000,  46.7500,  46.5000,\n",
      "         46.2500,  46.0000,  45.7500,  45.5000,  45.2500,  45.0000,  44.7500,\n",
      "         44.5000,  44.2500,  44.0000,  43.7500,  43.5000,  43.2500,  43.0000,\n",
      "         42.7500,  42.5000,  42.2500,  42.0000,  41.7500,  41.5000,  41.2500,\n",
      "         41.0000,  40.7500,  40.5000,  40.2500,  40.0000,  39.7500,  39.5000,\n",
      "         39.2500,  39.0000,  38.7500,  38.5000,  38.2500,  38.0000,  37.7500,\n",
      "         37.5000,  37.2500,  37.0000,  36.7500,  36.5000,  36.2500,  36.0000,\n",
      "         35.7500,  35.5000,  35.2500,  35.0000,  34.7500,  34.5000,  34.2500,\n",
      "         34.0000,  33.7500,  33.5000,  33.2500,  33.0000,  32.7500,  32.5000,\n",
      "         32.2500,  32.0000,  31.7500,  31.5000,  31.2500,  31.0000,  30.7500,\n",
      "         30.5000,  30.2500,  30.0000,  29.7500,  29.5000,  29.2500,  29.0000,\n",
      "         28.7500,  28.5000,  28.2500,  28.0000,  27.7500,  27.5000,  27.2500,\n",
      "         27.0000,  26.7500,  26.5000,  26.2500,  26.0000,  25.7500,  25.5000,\n",
      "         25.2500,  25.0000,  24.7500,  24.5000,  24.2500,  24.0000,  23.7500,\n",
      "         23.5000,  23.2500,  23.0000,  22.7500,  22.5000,  22.2500,  22.0000,\n",
      "         21.7500,  21.5000,  21.2500,  21.0000,  20.7500,  20.5000,  20.2500,\n",
      "         20.0000,  19.7500,  19.5000,  19.2500,  19.0000,  18.7500,  18.5000,\n",
      "         18.2500,  18.0000,  17.7500,  17.5000,  17.2500,  17.0000,  16.7500,\n",
      "         16.5000,  16.2500,  16.0000,  15.7500,  15.5000,  15.2500,  15.0000,\n",
      "         14.7500,  14.5000,  14.2500,  14.0000,  13.7500,  13.5000,  13.2500,\n",
      "         13.0000,  12.7500,  12.5000,  12.2500,  12.0000,  11.7500,  11.5000,\n",
      "         11.2500,  11.0000,  10.7500,  10.5000,  10.2500,  10.0000,   9.7500,\n",
      "          9.5000,   9.2500,   9.0000,   8.7500,   8.5000,   8.2500,   8.0000,\n",
      "          7.7500,   7.5000,   7.2500,   7.0000,   6.7500,   6.5000,   6.2500,\n",
      "          6.0000,   5.7500,   5.5000,   5.2500,   5.0000,   4.7500,   4.5000,\n",
      "          4.2500,   4.0000,   3.7500,   3.5000,   3.2500,   3.0000,   2.7500,\n",
      "          2.5000,   2.2500,   2.0000,   1.7500,   1.5000,   1.2500,   1.0000,\n",
      "          0.7500,   0.5000,   0.2500,   0.0000,  -0.2500,  -0.5000,  -0.7500,\n",
      "         -1.0000,  -1.2500,  -1.5000,  -1.7500,  -2.0000,  -2.2500,  -2.5000,\n",
      "         -2.7500,  -3.0000,  -3.2500,  -3.5000,  -3.7500,  -4.0000,  -4.2500,\n",
      "         -4.5000,  -4.7500,  -5.0000,  -5.2500,  -5.5000,  -5.7500,  -6.0000,\n",
      "         -6.2500,  -6.5000,  -6.7500,  -7.0000,  -7.2500,  -7.5000,  -7.7500,\n",
      "         -8.0000,  -8.2500,  -8.5000,  -8.7500,  -9.0000,  -9.2500,  -9.5000,\n",
      "         -9.7500, -10.0000, -10.2500, -10.5000, -10.7500, -11.0000, -11.2500,\n",
      "        -11.5000, -11.7500, -12.0000, -12.2500, -12.5000, -12.7500, -13.0000,\n",
      "        -13.2500, -13.5000, -13.7500, -14.0000, -14.2500, -14.5000, -14.7500,\n",
      "        -15.0000, -15.2500, -15.5000, -15.7500, -16.0000, -16.2500, -16.5000,\n",
      "        -16.7500, -17.0000, -17.2500, -17.5000, -17.7500, -18.0000, -18.2500,\n",
      "        -18.5000, -18.7500, -19.0000, -19.2500, -19.5000, -19.7500, -20.0000,\n",
      "        -20.2500, -20.5000, -20.7500, -21.0000, -21.2500, -21.5000, -21.7500,\n",
      "        -22.0000, -22.2500, -22.5000, -22.7500, -23.0000, -23.2500, -23.5000,\n",
      "        -23.7500, -24.0000, -24.2500, -24.5000, -24.7500, -25.0000, -25.2500,\n",
      "        -25.5000, -25.7500, -26.0000, -26.2500, -26.5000, -26.7500, -27.0000,\n",
      "        -27.2500, -27.5000, -27.7500, -28.0000, -28.2500, -28.5000, -28.7500,\n",
      "        -29.0000, -29.2500, -29.5000, -29.7500, -30.0000, -30.2500, -30.5000,\n",
      "        -30.7500, -31.0000, -31.2500, -31.5000, -31.7500, -32.0000, -32.2500,\n",
      "        -32.5000, -32.7500, -33.0000, -33.2500, -33.5000, -33.7500, -34.0000,\n",
      "        -34.2500, -34.5000, -34.7500, -35.0000, -35.2500, -35.5000, -35.7500,\n",
      "        -36.0000, -36.2500, -36.5000, -36.7500, -37.0000, -37.2500, -37.5000,\n",
      "        -37.7500, -38.0000, -38.2500, -38.5000, -38.7500, -39.0000, -39.2500,\n",
      "        -39.5000, -39.7500, -40.0000, -40.2500, -40.5000, -40.7500, -41.0000,\n",
      "        -41.2500, -41.5000, -41.7500, -42.0000, -42.2500, -42.5000, -42.7500,\n",
      "        -43.0000, -43.2500, -43.5000, -43.7500, -44.0000, -44.2500, -44.5000,\n",
      "        -44.7500, -45.0000, -45.2500, -45.5000, -45.7500, -46.0000, -46.2500,\n",
      "        -46.5000, -46.7500, -47.0000, -47.2500, -47.5000, -47.7500, -48.0000,\n",
      "        -48.2500, -48.5000, -48.7500, -49.0000, -49.2500, -49.5000, -49.7500,\n",
      "        -50.0000, -50.2500, -50.5000, -50.7500, -51.0000, -51.2500, -51.5000,\n",
      "        -51.7500, -52.0000, -52.2500, -52.5000, -52.7500, -53.0000, -53.2500,\n",
      "        -53.5000, -53.7500, -54.0000, -54.2500, -54.5000, -54.7500, -55.0000,\n",
      "        -55.2500, -55.5000, -55.7500, -56.0000, -56.2500, -56.5000, -56.7500,\n",
      "        -57.0000, -57.2500, -57.5000, -57.7500, -58.0000, -58.2500, -58.5000,\n",
      "        -58.7500, -59.0000, -59.2500, -59.5000, -59.7500, -60.0000, -60.2500,\n",
      "        -60.5000, -60.7500, -61.0000, -61.2500, -61.5000, -61.7500, -62.0000,\n",
      "        -62.2500, -62.5000, -62.7500, -63.0000, -63.2500, -63.5000, -63.7500,\n",
      "        -64.0000, -64.2500, -64.5000, -64.7500, -65.0000, -65.2500, -65.5000,\n",
      "        -65.7500, -66.0000, -66.2500, -66.5000, -66.7500, -67.0000, -67.2500,\n",
      "        -67.5000, -67.7500, -68.0000, -68.2500, -68.5000, -68.7500, -69.0000,\n",
      "        -69.2500, -69.5000, -69.7500, -70.0000, -70.2500, -70.5000, -70.7500,\n",
      "        -71.0000, -71.2500, -71.5000, -71.7500, -72.0000, -72.2500, -72.5000,\n",
      "        -72.7500, -73.0000, -73.2500, -73.5000, -73.7500, -74.0000, -74.2500,\n",
      "        -74.5000, -74.7500, -75.0000, -75.2500, -75.5000, -75.7500, -76.0000,\n",
      "        -76.2500, -76.5000, -76.7500, -77.0000, -77.2500, -77.5000, -77.7500,\n",
      "        -78.0000, -78.2500, -78.5000, -78.7500, -79.0000, -79.2500, -79.5000,\n",
      "        -79.7500, -80.0000, -80.2500, -80.5000, -80.7500, -81.0000, -81.2500,\n",
      "        -81.5000, -81.7500, -82.0000, -82.2500, -82.5000, -82.7500, -83.0000,\n",
      "        -83.2500, -83.5000, -83.7500, -84.0000, -84.2500, -84.5000, -84.7500,\n",
      "        -85.0000, -85.2500, -85.5000, -85.7500, -86.0000, -86.2500, -86.5000,\n",
      "        -86.7500, -87.0000, -87.2500, -87.5000, -87.7500, -88.0000, -88.2500,\n",
      "        -88.5000, -88.7500, -89.0000, -89.2500, -89.5000, -89.7500],\n",
      "       device='cuda:0'), lon=tensor([0.0000e+00, 2.5000e-01, 5.0000e-01,  ..., 3.5925e+02, 3.5950e+02,\n",
      "        3.5975e+02], device='cuda:0'), time=(datetime.datetime(2015, 1, 3, 12, 0),), atmos_levels=(1000, 925, 850, 700, 600, 500, 400, 300, 250, 200, 100, 50), rollout_step=1))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/torch/autograd/graph.py:825: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/MHA.cpp:674.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computed first batch\n",
      "Epoch [1/1], Loss: 0.0017\n"
     ]
    }
   ],
   "source": [
    "# model training \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "model.train()\n",
    "model.configure_activation_checkpointing()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-5, weight_decay=5e-6)\n",
    "\n",
    "model = model.to(device)\n",
    "model.train()\n",
    "model.configure_activation_checkpointing()\n",
    "torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "num_epochs = 1\n",
    "torch.cuda.empty_cache()\n",
    "for epoch in range(num_epochs):  # Loop over epochs\n",
    "    # model.train()  # Set model to training mode\n",
    "    for batch in dataloader:  # Iterate over batches\n",
    "        # Unpack the batch\n",
    "        features, labels = batch  # Ensure your Dataset and collate function return the right format\n",
    "        # Move data to the appropriate device\n",
    "        # features = {key: val.to(device) for key, val in features.items()}  # For dict-based features\n",
    "        labels = labels.to(device)\n",
    "        # features = features.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        # outputs = model(features)\n",
    "        pred = model.forward(features)\n",
    "        # pred is already on gpu \n",
    "        # pred = pred.to(device)\n",
    "        print(pred)\n",
    "        # Compute the loss\n",
    "        loss = loss_func(labels,pred)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "        loss.backward()  # Backpropagate\n",
    "        optimizer.step()  # Update the weights\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"computed first batch\")\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:256\"\n",
    "# !export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:64 3.17 112"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(os.getenv(\"PYTORCH_CUDA_ALLOC_CONF\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(torch.cuda.memory_summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predicting on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2476/1557875420.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"aurora_fire_weights_65.pth\"), strict=False)\n"
     ]
    }
   ],
   "source": [
    "model = Aurora(\n",
    "    use_lora=False,\n",
    "    autocast=True,      # reduces memory usage\n",
    "    surf_vars=(\"2t\", \"10u\", \"10v\", \"msl\", \"fire\",\"lst\"),\n",
    "    static_vars=(\"lsm\", \"z\", \"slt\"),\n",
    "    atmos_vars=(\"z\", \"u\", \"v\", \"t\", \"q\"),\n",
    ")\n",
    "# model = Aurora(\n",
    "#     use_lora=False,\n",
    "#     autocast=True,      # reduces memory usage\n",
    "#     # surf_vars=(\"2t\", \"10u\", \"10v\", \"msl\"),\n",
    "#     surf_vars=(\"2t\", \"10u\", \"10v\", \"msl\", \"fire\"),\n",
    "#     static_vars=(\"lsm\", \"z\", \"slt\"),\n",
    "#     atmos_vars=(\"z\", \"u\", \"v\", \"t\", \"q\"),\n",
    "# )\n",
    "\n",
    "# model.load_checkpoint(\"microsoft/aurora\", \"aurora-0.25-pretrained.ckpt\", strict=False)\n",
    "model.load_state_dict(torch.load(\"aurora_fire_weights_65.pth\"), strict=False)\n",
    "model = model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1.]\n",
      "[0. 1.]\n",
      "in getitem feature: [0. 1.]\n",
      "in getitem label: [0.         0.12847137 0.12985992 0.1340332  0.13542175 0.1368103\n",
      " 0.14375305 0.1451416  0.14653015 0.1479187  0.14930725 0.1506958\n",
      " 0.15208435 0.1534729  0.15486145 0.15625    0.15763855 0.1590271\n",
      " 0.16041565 0.16320038 0.16875458 0.17014313 0.17153168 0.17292023\n",
      " 0.17430878 0.17708588 0.17848206 0.18125916 0.1826477  0.18403625\n",
      " 0.1854248  0.18681335 0.1882019  0.18959045 0.190979   0.19236755\n",
      " 0.1937561  0.19514465 0.1965332  0.19792175 0.1993103  0.20069885\n",
      " 0.2020874  0.20347595 0.2048645  0.20625305 0.2076416  0.20903015\n",
      " 0.2104187  0.21180725 0.2131958  0.21458435 0.2159729  0.21736145\n",
      " 0.21875    0.22013855 0.2215271  0.22291565 0.2243042  0.22569275\n",
      " 0.2270813  0.22846985 0.2298584  0.23124695 0.2326355  0.23402405\n",
      " 0.2354126  0.23680115 0.2395935  0.2451477  0.24653625 0.2618103\n",
      " 0.2673645  0.27430725 0.2756958  0.2784729  0.27986145 0.28125\n",
      " 0.2895813  0.29096985 0.2923584  0.29374695 0.2951355  0.2965393\n",
      " 0.29792786 0.3006897  0.3062439  0.30625916 0.3076172  0.3076477\n",
      " 0.309021   0.30903625 0.3103943  0.3117981  0.3131714  0.3132019\n",
      " 0.3145752  0.31459045 0.3159485  0.3173523  0.31736755 0.3187561\n",
      " 0.3201294  0.32014465 0.3215332  0.3229065  0.32292175 0.3243103\n",
      " 0.3256836  0.32569885 0.3270874  0.3284607  0.32847595 0.3298645\n",
      " 0.3312378  0.33125305 0.3326416  0.33403015 0.3354187  0.336792\n",
      " 0.33680725 0.3381958  0.33958435 0.3409729  0.34236145 0.34375\n",
      " 0.3451233  0.34513855 0.3465271  0.3479004  0.34791565 0.3493042\n",
      " 0.3506775  0.35069275 0.3520813  0.3534546  0.35346985 0.3548584\n",
      " 0.3562622  0.3576355  0.3590088  0.3590393  0.3604126  0.3617859\n",
      " 0.3631897  0.364563   0.3659668  0.3673401  0.3673706  0.3687439\n",
      " 0.3701172  0.3701477  0.371521   0.3728943  0.3742981  0.3756714\n",
      " 0.3757019  0.3770752  0.378479   0.3798523  0.3812561  0.3826294\n",
      " 0.3840332  0.3854065  0.385437   0.3868103  0.3881836  0.3882141\n",
      " 0.3895874  0.3909607  0.3909912  0.3923645  0.3937378  0.3937683\n",
      " 0.3951416  0.3965149  0.3965454  0.3979187  0.399292   0.3993225\n",
      " 0.4006958  0.4020691  0.4020996  0.4034729  0.4048462  0.4048767\n",
      " 0.40625    0.4076233  0.4076538  0.4090271  0.4104004  0.4104309\n",
      " 0.4118042  0.4131775  0.413208   0.4145813  0.4159546  0.4159851\n",
      " 0.4173584  0.4187317  0.4187622  0.4201355  0.4215088  0.4215393\n",
      " 0.4229126  0.4242859  0.4243164  0.4256897  0.427063   0.4270935\n",
      " 0.4284668  0.4298401  0.4298706  0.4312439  0.4326172  0.4326477\n",
      " 0.434021   0.4353943  0.4354248  0.4367981  0.4382019  0.4395752\n",
      " 0.440979   0.4423523  0.4437561  0.4451294  0.4451599  0.4465332\n",
      " 0.4479065  0.447937   0.4493103  0.4506836  0.4507141  0.4520874\n",
      " 0.4534607  0.4534912  0.4548645  0.4562378  0.4562683  0.4576416\n",
      " 0.4590149  0.4590454  0.4604187  0.461792   0.4618225  0.4631958\n",
      " 0.4645691  0.4645996  0.4659729  0.4673462  0.4673767  0.46875\n",
      " 0.4701233  0.4701538  0.4715271  0.4729004  0.4729309  0.4743042\n",
      " 0.4756775  0.475708   0.4770813  0.4784546  0.4784851  0.4798584\n",
      " 0.4812317  0.4826355  0.4840088  0.4854126  0.4867859  0.489563\n",
      " 0.4895935  0.4923401  0.4937439  0.4979248  0.5007019  0.5048523\n",
      " 0.5062561  0.5090332  0.510437   0.5118103  0.5132141  0.5145874\n",
      " 0.5159607  0.5159912  0.5173645  0.5187683  0.5201416  0.5215149\n",
      " 0.5215454  0.5229187  0.524292   0.5243225  0.5256958  0.5270691\n",
      " 0.5270996  0.5284729  0.5298767  0.53125    0.5326233  0.5326538\n",
      " 0.5340271  0.5354004  0.5354309  0.5368042  0.5381775  0.538208\n",
      " 0.5395813  0.5409546  0.5409851  0.5423584  0.5437317  0.5437622\n",
      " 0.5451355  0.5465088  0.5465393  0.5479126  0.5492859  0.5493164\n",
      " 0.5506897  0.552063   0.5520935  0.5534668  0.5548401  0.5548706\n",
      " 0.5562439  0.5576477  0.559021   0.5604248  0.5617981  0.5632019\n",
      " 0.5645752  0.5646057  0.565979   0.5673523  0.5673828  0.5687561\n",
      " 0.5701294  0.5701599  0.5715332  0.5729065  0.572937   0.5743103\n",
      " 0.5756836  0.5757141  0.5770874  0.5784607  0.5784912  0.5798645\n",
      " 0.5812378  0.5812683  0.5826416  0.5840149  0.5840454  0.5854187\n",
      " 0.586792   0.5868225  0.5881958  0.5895691  0.5895996  0.5909729\n",
      " 0.5923462  0.5923767  0.59375    0.5951233  0.5951538  0.5965271\n",
      " 0.5979004  0.5979309  0.5993042  0.6006775  0.600708   0.6020813\n",
      " 0.6034546  0.6034851  0.6048584  0.6062317  0.6062622  0.6076355\n",
      " 0.6090088  0.6090393  0.6104126  0.6117859  0.6118164  0.6131897\n",
      " 0.614563   0.6145935  0.6159668  0.6173706  0.6187439  0.6201477\n",
      " 0.621521   0.6229248  0.6242981  0.6243286  0.6257019  0.6271057\n",
      " 0.628479   0.6298523  0.6298828  0.6312561  0.6326294  0.6326599\n",
      " 0.6340332  0.6354065  0.635437   0.6368103  0.6381836  0.6382141\n",
      " 0.6395874  0.6409607  0.6409912  0.6423645  0.6437378  0.6437683\n",
      " 0.6451416  0.64653015 0.6465454  0.6479187  0.64930725 0.6506958\n",
      " 0.6520996  0.6534729  0.65486145 0.6548767  0.65625    0.65763855\n",
      " 0.6590271  0.66041565 0.6604309  0.6618042  0.66319275 0.663208\n",
      " 0.6645813  0.66596985 0.6659851  0.6673584  0.66874695 0.6701355\n",
      " 0.6715393  0.6729126  0.67430115 0.6743164  0.6756897  0.67707825\n",
      " 0.6770935  0.6784668  0.6798706  0.6812439  0.6826477  0.684021\n",
      " 0.6840515  0.68540955 0.6854248  0.6868286  0.6882019  0.6895752\n",
      " 0.6896057  0.69096375 0.690979   0.6923523  0.6923828  0.697937\n",
      " 0.69929504 0.7006836  0.70207214 0.7034607  0.70625305 0.7076416\n",
      " 0.70903015 0.71736145 0.71875    0.72013855 0.7215271  0.72291565\n",
      " 0.73124695 0.73680115 0.7520752  0.75346375 0.75901794 0.76179504\n",
      " 0.76319885 0.7645874  0.76597595 0.7673645  0.76875305 0.7701416\n",
      " 0.77153015 0.7729187  0.77430725 0.7756958  0.77708435 0.7784729\n",
      " 0.77986145 0.78125    0.78263855 0.7840271  0.78541565 0.7868042\n",
      " 0.78819275 0.7895813  0.79096985 0.7923584  0.79374695 0.7951355\n",
      " 0.79652405 0.7979126  0.79930115 0.8006897  0.80207825 0.8034668\n",
      " 0.80485535 0.8062439  0.80763245 0.809021   0.81040955 0.8117981\n",
      " 0.81318665 0.8145752  0.81596375 0.8173523  0.8201294  0.82151794\n",
      " 0.8243027  0.8256912  0.8270798  0.8284683  0.8298569  0.8354111\n",
      " 0.8381958  0.83958435 0.8409729  0.84236145 0.84375    0.84513855\n",
      " 0.8465271  0.84791565 0.8493042  0.85069275 0.85346985 0.8548584\n",
      " 0.86180115 0.8631897  0.86457825 0.8687515  0.8701401  1.        ]\n",
      "label unique: [0.         0.12847137 0.12985992 0.1340332  0.13542175 0.1368103\n",
      " 0.14375305 0.1451416  0.14653015 0.1479187 ]\n",
      "feature unique: [0. 1.]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacity of 94.50 GiB of which 12.19 MiB is free. Including non-PyTorch memory, this process has 94.48 GiB memory in use. Of the allocated memory 93.64 GiB is allocated by PyTorch, and 73.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2476/952225432.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/aurora/batch.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"Batch\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0;34m\"\"\"Move the batch to another device.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"Batch\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/aurora/batch.py\u001b[0m in \u001b[0;36m_fmap\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0msurf_vars\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msurf_vars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0mstatic_vars\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatic_vars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m             \u001b[0matmos_vars\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matmos_vars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m             metadata=Metadata(\n\u001b[1;32m    176\u001b[0m                 \u001b[0mlat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/aurora/batch.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0msurf_vars\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msurf_vars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0mstatic_vars\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatic_vars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m             \u001b[0matmos_vars\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matmos_vars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m             metadata=Metadata(\n\u001b[1;32m    176\u001b[0m                 \u001b[0mlat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/aurora/batch.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"Batch\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0;34m\"\"\"Move the batch to another device.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"Batch\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacity of 94.50 GiB of which 12.19 MiB is free. Including non-PyTorch memory, this process has 94.48 GiB memory in use. Of the allocated memory 93.64 GiB is allocated by PyTorch, and 73.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, jaccard_score\n",
    "\n",
    "accuracy = 0\n",
    "precision = 0\n",
    "recall = 0\n",
    "f1 = 0\n",
    "iou = 0\n",
    "total_samples = 0\n",
    "\n",
    "threshold = 0.5 \n",
    "\n",
    "model.eval()  # Set model to evaluation mode\n",
    "with torch.no_grad():  # Disable gradient computations\n",
    "    # for batch in dataloader_test:\n",
    "    for batch in dataloader:\n",
    "        \n",
    "        features, labels = batch  # Ensure your Dataset and collate function return the right format\n",
    "        \n",
    "\n",
    "        print(\"label unique:\", np.unique(labels.surf_vars[\"fire\"].cpu().numpy())[:10])\n",
    "        print(\"feature unique:\",np.unique(features.surf_vars[\"fire\"].cpu().numpy()))\n",
    "\n",
    "        labels = labels.to(device)\n",
    "        features = features.to(device)\n",
    "        \n",
    "        pred = model.forward(features)\n",
    "        # pred = pred.to(device)\n",
    "        pred_fire = pred.surf_vars[\"fire\"].cpu().numpy().flatten()\n",
    "        pred_fire = np.nan_to_num(pred_fire, nan=0)\n",
    "        lab_fire = labels.surf_vars[\"fire\"].cpu().numpy().flatten()\n",
    "        # lab_fire = (lab_fire > threshold).astype(int)\n",
    "        \n",
    "        print(\"lab_fire:\", lab_fire.shape, lab_fire.dtype, lab_fire)\n",
    "        print(\"pred_fire:\", pred_fire.shape, pred_fire.dtype, pred_fire)\n",
    "        print(len(np.unique(lab_fire)))\n",
    "        print((np.unique(lab_fire)))\n",
    "        print(len(np.unique(pred_fire)))\n",
    "\n",
    "        # Assume `preds` and `labels` are your predicted and true grids (flattened for metrics)\n",
    "        # pred_fire = pred_fire.cpu().numpy().flatten()\n",
    "        # lab_fire = lab_fire.cpu().numpy().flatten()\n",
    "        # pred_fire = pred_fire.numpy().flatten()\n",
    "        # lab_fire = lab_fire.numpy().flatten()\n",
    "\n",
    "        # Compute metrics\n",
    "        accuracy += accuracy_score(lab_fire, pred_fire)\n",
    "        precision += precision_score(lab_fire, pred_fire, zero_division=1)\n",
    "        recall += recall_score(lab_fire, pred_fire, zero_division=1)\n",
    "        f1 += f1_score(lab_fire, pred_fire, zero_division=1)\n",
    "        iou += jaccard_score(lab_fire, pred_fire)\n",
    "        # total_samples += labels.size(0)\n",
    "        total_samples += 1\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"IoU: {iou:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 2\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7116/2164241448.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"aurora_fire_weights_65.pth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m### see aws id for missing line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"aurora_fire_weights_65.pth\")\n",
    "\n",
    "### see aws id for missing line\n",
    "# bucket_name = 'globfire-gooddata'\n",
    "# s3.upload_file(\"aurora_fire_weights.pth\", bucket_name, \"aurora_fire_weights.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ArQeaY0b8iht"
   },
   "outputs": [],
   "source": [
    "from aurora import Aurora\n",
    "\n",
    "model = Aurora(\n",
    "    use_lora=False,  # Model was not fine-tuned.\n",
    "    autocast=True,  # Use AMP.\n",
    ")\n",
    "model.load_checkpoint(\"microsoft/aurora\", \"aurora-0.25-pretrained.ckpt\")\n",
    "\n",
    "batch = ...  # Load some data.\n",
    "\n",
    "model = model.cuda()\n",
    "model.train()\n",
    "model.configure_activation_checkpointing()\n",
    "\n",
    "pred = model.forward(batch)\n",
    "loss = ...\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## trying to see if thing works with generic aah data 721 - 720 check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "static_vars_ds = xr.open_dataset(f\"../{download_path}/static/static.nc\", engine=\"netcdf4\")\n",
    "surf_vars_ds = xr.open_dataset(f\"../{download_path}/fle/surf_2015-01-04.nc\", engine=\"netcdf4\")\n",
    "atmos_vars_ds = xr.open_dataset(f\"../{download_path}/atmospheric/201501/atmospheric_20150104.nc\", engine=\"netcdf4\")\n",
    "\n",
    "\n",
    "i = 0  # Select this time index in the downloaded data.\n",
    "\n",
    "batch3 = Batch(\n",
    "    surf_vars={\n",
    "        # First select time points `i` and `i - 1`. Afterwards, `[None]` inserts a\n",
    "        # batch dimension of size one.\n",
    "        \"2t\": torch.from_numpy(surf_vars_ds[\"t2m\"].values[[i - 1, i]][None]),\n",
    "        \"10u\": torch.from_numpy(surf_vars_ds[\"u10\"].values[[i - 1, i]][None]),\n",
    "        \"10v\": torch.from_numpy(surf_vars_ds[\"v10\"].values[[i - 1, i]][None]),\n",
    "        \"msl\": torch.from_numpy(surf_vars_ds[\"msl\"].values[[i - 1, i]][None])\n",
    "    },\n",
    "    static_vars={\n",
    "        # The static variables are constant, so we just get them for the first time.\n",
    "        \"z\": torch.from_numpy(static_vars_ds[\"z\"].values[0]),\n",
    "        \"slt\": torch.from_numpy(static_vars_ds[\"slt\"].values[0]),\n",
    "        \"lsm\": torch.from_numpy(static_vars_ds[\"lsm\"].values[0]),\n",
    "    },\n",
    "    atmos_vars={\n",
    "        \"t\": torch.from_numpy(atmos_vars_ds[\"t\"].values[[i - 1, i]][None]),\n",
    "        \"u\": torch.from_numpy(atmos_vars_ds[\"u\"].values[[i - 1, i]][None]),\n",
    "        \"v\": torch.from_numpy(atmos_vars_ds[\"v\"].values[[i - 1, i]][None]),\n",
    "        \"q\": torch.from_numpy(atmos_vars_ds[\"q\"].values[[i - 1, i]][None]),\n",
    "        \"z\": torch.from_numpy(atmos_vars_ds[\"z\"].values[[i - 1, i]][None])\n",
    "    },\n",
    "    metadata=Metadata(\n",
    "        lat=torch.from_numpy(surf_vars_ds.latitude.values),\n",
    "        lon=torch.from_numpy(surf_vars_ds.longitude.values),\n",
    "        # Converting to `datetime64[s]` ensures that the output of `tolist()` gives\n",
    "        # `datetime.datetime`s. Note that this needs to be a tuple of length one:\n",
    "        # one value for every batch element.\n",
    "        time=(surf_vars_ds.valid_time.values.astype(\"datetime64[s]\").tolist()[i],),\n",
    "        atmos_levels=tuple(int(level) for level in atmos_vars_ds.pressure_level.values),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0  # Select this time index in the downloaded data.\n",
    "\n",
    "batch4 = Batch(\n",
    "    surf_vars={\n",
    "        # First select time points `i` and `i - 1`. Afterwards, `[None]` inserts a\n",
    "        # batch dimension of size one.\n",
    "        \"2t\": torch.from_numpy(surf_comb[\"t2m\"].values[[i - 1, i]][None]),\n",
    "        \"10u\": torch.from_numpy(surf_comb[\"u10\"].values[[i - 1, i]][None]),\n",
    "        \"10v\": torch.from_numpy(surf_comb[\"v10\"].values[[i - 1, i]][None]),\n",
    "        \"msl\": torch.from_numpy(surf_comb[\"msl\"].values[[i - 1, i]][None]),\n",
    "    },\n",
    "    static_vars={\n",
    "        # The static variables are constant, so we just get them for the first time.\n",
    "        \"z\": torch.from_numpy(static_vars_ds[\"z\"].values[0]),\n",
    "        \"slt\": torch.from_numpy(static_vars_ds[\"slt\"].values[0]),\n",
    "        \"lsm\": torch.from_numpy(static_vars_ds[\"lsm\"].values[0]),\n",
    "    },\n",
    "    atmos_vars={\n",
    "        \"t\": torch.from_numpy(atmos_comb[\"t\"].values[[i - 1, i]][None]),\n",
    "        \"u\": torch.from_numpy(atmos_comb[\"u\"].values[[i - 1, i]][None]),\n",
    "        \"v\": torch.from_numpy(atmos_comb[\"v\"].values[[i - 1, i]][None]),\n",
    "        \"q\": torch.from_numpy(atmos_comb[\"q\"].values[[i - 1, i]][None]),\n",
    "        \"z\": torch.from_numpy(atmos_comb[\"z\"].values[[i - 1, i]][None]),\n",
    "    },\n",
    "    metadata=Metadata(\n",
    "        lat=torch.from_numpy(surf_comb.latitude.values),\n",
    "        lon=torch.from_numpy(surf_comb.longitude.values),\n",
    "        # Converting to `datetime64[s]` ensures that the output of `tolist()` gives\n",
    "        # `datetime.datetime`s. Note that this needs to be a tuple of length one:\n",
    "        # one value for every batch element.\n",
    "        time=(surf_comb.valid_time.values.astype(\"datetime64[s]\").tolist()[i],),\n",
    "        atmos_levels=tuple(int(level) for level in atmos_comb.pressure_level.values),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aurora import Aurora, rollout\n",
    "\n",
    "model2 = Aurora(use_lora=False)  # The pretrained version does not use LoRA.\n",
    "model2.load_checkpoint(\"microsoft/aurora\", \"aurora-0.25-pretrained.ckpt\")\n",
    "\n",
    "model2.eval()\n",
    "model2 = model2.to(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with torch.inference_mode():\n",
    "    preds = [pred.to(\"cpu\") for pred in rollout(model2, batch4, steps=2)]\n",
    "\n",
    "model2 = model2.to(\"cpu\")\n",
    "\n",
    "# torch.stack(list(pred.surf_vars.values())).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds[1].surf_vars['2t'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
